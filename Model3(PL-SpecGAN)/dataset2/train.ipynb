{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training1 \n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 調整參數\n",
    "fs = 400000 #400k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cyclos data\n",
    "import csv\n",
    "import numpy as np\n",
    "real_datas = []\n",
    "with open('cyclo2_fresh.csv', newline='') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    for row in rows:\n",
    "        real_datas.append( list( map(float,row) ) )\n",
    "    csvfile.close()\n",
    "real_datas = np.array(real_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "print( stats.describe(real_datas[0]) )\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(np.arange(0,len(real_datas[1])/fs,1/fs),real_datas[10])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "#fs = 360000\n",
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "cm=plt.cm.get_cmap('rainbow')\n",
    "freq, times, Zxx = signal.stft(real_datas[0], fs=fs, nperseg=257, noverlap=128)\n",
    "freq = freq[1:]\n",
    "Zxx = Zxx[1:]\n",
    "\n",
    "# handle abs \n",
    "lownum = 1e-8\n",
    "Zxx2 = abs(Zxx) + lownum\n",
    "Zxx2log = np.log10(Zxx2)\n",
    "minva = -4\n",
    "Zxx2nor = ( Zxx2log - (minva) )/abs(minva)\n",
    "plt.pcolormesh(times, freq, Zxx2nor,cmap=cm)\n",
    "\n",
    "# handle the angle\n",
    "phase = np.angle(Zxx)\n",
    "finetuneangle = 3.15\n",
    "tophase = phase / finetuneangle\n",
    "\n",
    "\n",
    "print(np.max(Zxx2nor),np.min(Zxx2nor),np.mean(Zxx2nor))\n",
    "print(np.max(tophase),np.min(tophase),np.mean(tophase))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(Zxx2),np.min(Zxx2),np.mean(Zxx2))\n",
    "print(np.max(Zxx2log),np.min(Zxx2log),np.mean(Zxx2log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training start\n",
    "import Ipynb_importer\n",
    "# %load train.py\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch import optim\n",
    "import json\n",
    "#from utils import save_samples\n",
    "import numpy as np\n",
    "import pprint\n",
    "import pickle\n",
    "import datetime\n",
    "from specgan import *\n",
    "from utils import *\n",
    "from logger import *\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============Logger===============\n",
    "LOGGER = logging.getLogger('wavegan')\n",
    "LOGGER.setLevel(logging.DEBUG)\n",
    "\n",
    "LOGGER.info('Initialized logger.')\n",
    "init_console_logger(LOGGER)\n",
    "\n",
    "# =============Parameters===============\n",
    "args = parse_arguments()\n",
    "epochs = args['num_epochs']\n",
    "batch_size = args['batch_size']\n",
    "latent_dim = args['latent_dim']\n",
    "ngpus = args['ngpus']\n",
    "model_size = args['model_size']\n",
    "model_dir = make_path(os.path.join(args['output_dir'],\n",
    "                                   datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")))\n",
    "args['model_dir'] = model_dir\n",
    "# save samples for every N epochs.\n",
    "epochs_per_sample = args['epochs_per_sample']\n",
    "# gradient penalty regularization factor.\n",
    "lmbda = args['lmbda']\n",
    "\n",
    "# Dir\n",
    "audio_dir = args['audio_dir']\n",
    "output_dir = args['output_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = SpecGANGenerator(model_size=model_size, ngpus=ngpus, latent_dim=latent_dim, upsample=True)\n",
    "netD = SpecGANDiscriminator(model_size=model_size, ngpus=ngpus)\n",
    "\n",
    "if cuda:\n",
    "    netG = torch.nn.DataParallel(netG).cuda()\n",
    "    netD = torch.nn.DataParallel(netD).cuda()\n",
    "    \n",
    "#netG_path = os.path.join(output_dir, \"200generator.pkl\")\n",
    "#netD_path = os.path.join(output_dir, \"200discriminator.pkl\")\n",
    "\n",
    "#netG.load_state_dict(torch.load(netG_path), strict=False)\n",
    "#netD.load_state_dict(torch.load(netD_path), strict=False)\n",
    "\n",
    "# \"Two time-scale update rule\"(TTUR) to update netD 4x faster than netG.\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args['learning_rate'], betas=(args['beta1'], args['beta2']))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=args['learning_rate'], betas=(args['beta1'], args['beta2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample noise used for generated output.\n",
    "sample_noise = torch.randn(args['sample_size'], latent_dim)\n",
    "if cuda:\n",
    "    sample_noise = sample_noise.cuda()\n",
    "sample_noise_Var = autograd.Variable(sample_noise, requires_grad=False)\n",
    "\n",
    "# Save config.\n",
    "LOGGER.info('Saving configurations...')\n",
    "config_path = os.path.join(model_dir, 'config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(args, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER.info('Loading audio data...')\n",
    "\n",
    "# Load cs09 data #\n",
    "# audio_paths = get_all_audio_filepaths(audio_dir) #list\n",
    "# train_data, valid_data, test_data, train_size = split_data(audio_paths, args['valid_ratio'],\n",
    "#                                                            args['test_ratio'], batch_size)\n",
    "\n",
    "# Load noise data #\n",
    "num_ = len(real_datas)\n",
    "num_valid = int(np.ceil(num_ * args['valid_ratio']))\n",
    "num_train = num_ - num_valid\n",
    "train_size = num_train\n",
    "if not (num_valid > 0 and num_train > 0):\n",
    "    LOGGER.error(\"Please download DATASET !\")\n",
    "train_data = batch_generator( real_datas[:num_train],batch_size )\n",
    "valid_data = batch_generator( real_datas[num_train:],batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TRAIN_SAMPLES = train_size\n",
    "BATCH_NUM = TOTAL_TRAIN_SAMPLES // batch_size\n",
    "train_iter = iter(train_data)\n",
    "valid_iter = iter(valid_data)\n",
    "#test_iter = iter(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "D_costs_train = []\n",
    "D_wasses_train = []\n",
    "D_costs_valid = []\n",
    "D_wasses_valid = []\n",
    "G_costs = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "LOGGER.info('Starting training...EPOCHS={}, BATCH_SIZE={}, BATCH_NUM={}'.format(epochs, batch_size, BATCH_NUM))\n",
    "for epoch in range(epochs):\n",
    "    LOGGER.info(\"{} Epoch: {}/{}\".format(time_since(start), epoch, epochs))\n",
    "\n",
    "    D_cost_train_epoch = []\n",
    "    D_wass_train_epoch = []\n",
    "    D_cost_valid_epoch = []\n",
    "    D_wass_valid_epoch = []\n",
    "    G_cost_epoch = []\n",
    "    for i in range(1, BATCH_NUM+1):\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        one = torch.tensor(1, dtype=torch.float)\n",
    "        neg_one = one * -1\n",
    "        if cuda:\n",
    "            one = one.cuda()\n",
    "            neg_one = neg_one.cuda()\n",
    "\n",
    "        #######################################################\n",
    "        # (3) Train Discriminator\n",
    "        #######################################################\n",
    "        for iter_dis in range(5): # 5D 1G\n",
    "            netD.zero_grad()\n",
    "            noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
    "            if cuda:\n",
    "                noise = noise.cuda()\n",
    "            noise_Var = Variable(noise, requires_grad=False)\n",
    "            real_data_Var = numpy_to_var(next(train_iter)['X'], cuda) \n",
    "            # a) compute loss contribution from real training data\n",
    "            D_real = netD(real_data_Var)\n",
    "            D_real = D_real.mean()  # avg loss \n",
    "            D_real.backward(neg_one)  # loss * -1    \n",
    "            \n",
    "            # b) compute loss contribution from generated data, then backprop.\n",
    "            fake = autograd.Variable(netG(noise_Var).data)\n",
    "            #print(np.shape(fake))\n",
    "            D_fake = netD(fake)\n",
    "            D_fake = D_fake.mean()\n",
    "            D_fake.backward(one)\n",
    "            \n",
    "            # c) compute gradient penalty and backprop\n",
    "            gradient_penalty = calc_gradient_penalty(netD, real_data_Var.data,\n",
    "                                                     fake.data, batch_size, lmbda,\n",
    "                                                     use_cuda=cuda)\n",
    "            gradient_penalty.backward(one)\n",
    "\n",
    "            # Compute cost * Wassertein loss..\n",
    "            D_cost_train = D_fake - D_real + gradient_penalty\n",
    "            D_wass_train = D_real - D_fake \n",
    "            \n",
    "            # Update gradient of discriminator.\n",
    "            optimizerD.step()\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            #############################\n",
    "            # (2) Compute Valid data\n",
    "            #############################\n",
    "            netD.zero_grad()\n",
    "\n",
    "            valid_data_Var = numpy_to_var(next(valid_iter)['X'], cuda)\n",
    "            D_real_valid = netD(valid_data_Var)\n",
    "            D_real_valid = D_real_valid.mean()  # avg loss\n",
    "\n",
    "            # b) compute loss contribution from generated data, then backprop.\n",
    "            fake_valid = netG(noise_Var)\n",
    "            D_fake_valid = netD(fake_valid)\n",
    "            D_fake_valid = D_fake_valid.mean()\n",
    "\n",
    "            # c) compute gradient penalty and backprop\n",
    "            gradient_penalty_valid = calc_gradient_penalty(netD, valid_data_Var.data,\n",
    "                                                           fake_valid.data, batch_size, lmbda,\n",
    "                                                           use_cuda=cuda)\n",
    "            # Compute metrics and record in batch history.\n",
    "            D_cost_valid = D_fake_valid - D_real_valid + gradient_penalty_valid\n",
    "            D_wass_valid = D_real_valid - D_fake_valid\n",
    "\n",
    "            if cuda:\n",
    "                D_cost_train = D_cost_train.cpu()\n",
    "                D_wass_train = D_wass_train.cpu()\n",
    "                D_cost_valid = D_cost_valid.cpu()\n",
    "                D_wass_valid = D_wass_valid.cpu()\n",
    "\n",
    "            # Record costs\n",
    "            D_cost_train_epoch.append(D_cost_train.data.numpy())\n",
    "            D_wass_train_epoch.append(D_wass_train.data.numpy())\n",
    "            D_cost_valid_epoch.append(D_cost_valid.data.numpy())\n",
    "            D_wass_valid_epoch.append(D_wass_valid.data.numpy())\n",
    "            \n",
    "\n",
    "        #######################################################\n",
    "        # (3) Train Generator\n",
    "        #######################################################\n",
    "        # Prevent discriminator update.\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Reset generator gradients\n",
    "        netG.zero_grad()\n",
    "\n",
    "        # Noise\n",
    "        noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
    "        if cuda:\n",
    "            noise = noise.cuda()\n",
    "        noise_Var = Variable(noise, requires_grad=False) \n",
    "\n",
    "        fake = netG(noise_Var)\n",
    "        G = netD(fake)\n",
    "        G = G.mean()\n",
    "\n",
    "        # Update gradients.\n",
    "        G.backward(neg_one)\n",
    "        G_cost = -G\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Record costs\n",
    "        if cuda:\n",
    "            G_cost = G_cost.cpu()\n",
    "        G_cost_epoch.append(G_cost.data.numpy())\n",
    "\n",
    "        LOGGER.info(\"{} Epoch={} Batch: {}/{} D_c:{:.4f} | D_w:{:.4f} | G:{:.4f}\".format(time_since(start), epoch,\n",
    "                                                                                             i, BATCH_NUM,\n",
    "                                                                                             D_cost_train.data.numpy(),\n",
    "                                                                                             D_wass_train.data.numpy(),\n",
    "                                                                                             G_cost.data.numpy()))\n",
    "\n",
    "\n",
    "    # Save the average cost of batches in every epoch.\n",
    "    D_cost_train_epoch_avg = sum(D_cost_train_epoch) / float(len(D_cost_train_epoch))\n",
    "    D_wass_train_epoch_avg = sum(D_wass_train_epoch) / float(len(D_wass_train_epoch))\n",
    "    D_cost_valid_epoch_avg = sum(D_cost_valid_epoch) / float(len(D_cost_valid_epoch))\n",
    "    D_wass_valid_epoch_avg = sum(D_wass_valid_epoch) / float(len(D_wass_valid_epoch))\n",
    "    G_cost_epoch_avg = sum(G_cost_epoch) / float(len(G_cost_epoch))\n",
    "\n",
    "    D_costs_train.append(D_cost_train_epoch_avg)\n",
    "    D_wasses_train.append(D_wass_train_epoch_avg)\n",
    "    D_costs_valid.append(D_cost_valid_epoch_avg)\n",
    "    D_wasses_valid.append(D_wass_valid_epoch_avg)\n",
    "    G_costs.append(G_cost_epoch_avg)\n",
    "    \n",
    "\n",
    "    LOGGER.info(\"{} D_cost_train:{:.4f} | D_wass_train:{:.4f} | D_cost_valid:{:.4f} | D_wass_valid:{:.4f} | \"\n",
    "                \"G_cost:{:.4f}\".format(time_since(start),\n",
    "                                       D_cost_train_epoch_avg,\n",
    "                                       D_wass_train_epoch_avg,\n",
    "                                       D_cost_valid_epoch_avg,\n",
    "                                       D_wass_valid_epoch_avg,\n",
    "                                       G_cost_epoch_avg))\n",
    "    \n",
    "\n",
    "    #Generate audio samples.\n",
    "    if epoch % epochs_per_sample == 0:\n",
    "        LOGGER.info(\"Generating samples...\")\n",
    "        sample_out = netG(sample_noise_Var)  # sample_noise_Var = (10,100)\n",
    "        if cuda:\n",
    "            sample_out = sample_out.cpu()\n",
    "        sample_out = sample_out.data.numpy() # sample_out = (10,2,128,128)\n",
    "        # save spectrogram and timeseries and statistic\n",
    "        sample_dir = save_and_plot( times, freq, sample_out, epoch, output_dir )\n",
    "        \n",
    "    if epoch % 40 ==0:\n",
    "        # Save model\n",
    "        LOGGER.info(\"Saving models...\")\n",
    "        netD_path = os.path.join(output_dir, str(epoch)+\"discriminator.pkl\")\n",
    "        netG_path = os.path.join(output_dir, str(epoch)+\"generator.pkl\")\n",
    "        torch.save(netD.state_dict(), netD_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        torch.save(netG.state_dict(), netG_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        \n",
    "        \n",
    "LOGGER.info('>>>>>>>Training finished !<<<<<<<')\n",
    "\n",
    "# Save model\n",
    "LOGGER.info(\"Saving models...\")\n",
    "netD_path = os.path.join(output_dir, \"discriminator.pkl\")\n",
    "netG_path = os.path.join(output_dir, \"generator.pkl\")\n",
    "torch.save(netD.state_dict(), netD_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "torch.save(netG.state_dict(), netG_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Plot loss curve.\n",
    "LOGGER.info(\"Saving loss curve...\")\n",
    "plot_loss(D_costs_train, D_wasses_train,\n",
    "          D_costs_valid, D_wasses_valid, G_costs, output_dir)\n",
    "\n",
    "LOGGER.info(\"All finished!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER.info(\"Generating samples...\")\n",
    "fakee = []\n",
    "for i in range(245):\n",
    "    noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
    "    if cuda:\n",
    "        noise = noise.cuda()\n",
    "    noise_Var = Variable(noise, requires_grad=False)\n",
    "    sample_out = netG(noise_Var)  \n",
    "    if cuda:\n",
    "        sample_out = sample_out.cpu()\n",
    "    sample_out = sample_out.data.numpy() # sample_out = (10,2,128,128)\n",
    "    #sample_out = spectrogram_to_time(sample_out)[1]\n",
    "    fakee.append(sample_out)\n",
    "fakee = np.reshape( np.array(fakee),(-1,2,128,128) )\n",
    "print(fakee.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fakee[1][1].shape)\n",
    "print(np.max(fakee[1][0]))\n",
    "print(np.min(fakee[1][0]))\n",
    "print(np.mean(fakee[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cm=plt.cm.get_cmap('plasma')\n",
    "plt.figure(figsize=(20,10))\n",
    "#plt.title(\" X:Time[s], Y:Frequency[KHz]\")\n",
    "plt.pcolormesh( fakee[1][1],cmap=cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_datas = []\n",
    "for i in range(15000):\n",
    "    print(i)\n",
    "    time_re = spectrogram_to_time(fakee[i])[1] #[1] is really signal \n",
    "    fake_datas.append( time_re )\n",
    "fake_datas = np.array(fake_datas)\n",
    "print(fake_datas.shape)\n",
    "plt.plot(fake_datas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### validate ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "#real_datas = np.array(real_datas[:15000])\n",
    "fake_datas = np.array(fake_datas[:15000])\n",
    "#print(real_datas.shape)\n",
    "print(fake_datas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = \"metrice\"\n",
    "if not os.path.isdir(save):\n",
    "    os.makedirs(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store random 100 datas to generate CPS and coherence\n",
    "\n",
    "a = save + \"/\" + \"spec_improve_ep200\" + \"(cyclo2_fresh)\" + \".csv\"\n",
    "with open(a,'w',newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    for i in range(len(fake_datas)):\n",
    "        csvWriter.writerow( fake_datas[i] ) \n",
    "        \n",
    "\n",
    "        \n",
    "# b = save + \"/\" + \"real\" + \"(cyclo2_fresh)\" + \".csv\"\n",
    "# with open(b,'w',newline='') as csvFile:\n",
    "#     csvWriter = csv.writer(csvFile)\n",
    "#     for i in x:\n",
    "#         csvWriter.writerow( real_datas[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paremeter\n",
    "fs = 400000 # 400k\n",
    "data_length = len(real_datas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw time series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#fs = 360000 # 360k\n",
    "t = data_length / fs # sampling freq * sampling time = sample num\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot( np.arange(0,t,1/fs), real_datas[500], label = 'Real',color = (1,0,0))\n",
    "plt.legend(loc = 'upper right',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Noise amplitude(v)\",fontsize=16)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot( np.arange(0,t,1/fs), fake_datas[2], label = 'Generate')\n",
    "plt.legend(loc = 'upper right',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Noise amplitude(v)\",fontsize=16)\n",
    "\n",
    "\n",
    "plt.savefig(save + '/' + \"time_series.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.where( fake_datas[2]>0.09) : \n",
    "    print(i/400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(1,15000, size=(10))\n",
    "tot = 1\n",
    "# draw random 10 time series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#fs = 360000 # 360k\n",
    "t = data_length / fs # sampling freq * sampling time = sample num\n",
    "\n",
    "plt.figure(figsize=(15,50))\n",
    "for i in x:\n",
    "    plt.subplot(20,1,tot)\n",
    "    plt.plot( np.arange(0,t,1/fs), real_datas[i], label = 'Real',color = (1,0,0))\n",
    "    plt.legend(loc = 'upper right',fontsize=16)\n",
    "    plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "    plt.ylabel(\"Noise amplitude(v)\",fontsize=10)\n",
    "    \n",
    "    tot += 1\n",
    "    \n",
    "    plt.subplot(20,1,tot)\n",
    "    plt.plot( np.arange(0,t,1/fs), fake_datas[i], label = 'Generate')\n",
    "    plt.legend(loc = 'upper right',fontsize=16)\n",
    "    plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "    plt.ylabel(\"Noise amplitude(v)\",fontsize=10)\n",
    "    \n",
    "    tot += 1\n",
    "    \n",
    "plt.savefig(save + '/' + \"time_series(10random).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw stft\n",
    "from scipy import signal\n",
    "# fs = 360000\n",
    "f1, t1, Zxx1 = signal.stft(real_datas[500], fs=fs, nperseg=257, noverlap=128)\n",
    "f2, t2, Zxx2 = signal.stft(fake_datas[1], fs=fs, nperseg=257, noverlap=128)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "cm=plt.cm.get_cmap('plasma')\n",
    "plt.figure(figsize=(20,10))\n",
    "#plt.title(\" X:Time[s], Y:Frequency[KHz]\")\n",
    "plt.subplot(2,1,1)\n",
    "plt.pcolormesh(t1, f1/1000, abs(Zxx1),cmap=cm)\n",
    "plt.colorbar()\n",
    "#plt.legend(loc = 'best',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Frequency(kHz)\",fontsize=16)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.pcolormesh(t2, f2/1000, abs(Zxx2),cmap=cm)\n",
    "plt.colorbar()\n",
    "#plt.legend(loc = 'best',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Frequency(kHz)\",fontsize=16)\n",
    "plt.savefig(save + '/' + \"stft.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check power distribution\n",
    "intensity = 0.015\n",
    "\n",
    "print( np.where(abs(Zxx1)>intensity) )\n",
    "print(\"\\n\")\n",
    "i,j = np.where(abs(Zxx1)>intensity)\n",
    "print(\"frequency : \",f1[i]/1000)\n",
    "print(\"\\n\")\n",
    "print(\"time : \",t1[j])\n",
    "print(\"\\n\")\n",
    "print(\"time_sorted :\",sorted(t1[j]*1000))\n",
    "print(\"\\n\")\n",
    "\n",
    "print( np.where(abs(Zxx2)>intensity) )\n",
    "print(\"\\n\")\n",
    "i,j = np.where(abs(Zxx2)>intensity)\n",
    "print(\"frequency : \",f2[i]/1000)\n",
    "print(\"\\n\")\n",
    "print(\"time : \",t2[j])\n",
    "print(\"\\n\")\n",
    "print(\"time_sorted :\",sorted(t2[j]*1000))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft,ifft\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "le = len(real_datas[2])\n",
    "xf = np.linspace(0,fs,data_length) # 頻率\n",
    "xf2 = xf[:(le//2)]\n",
    "r_yf2 = abs(fft(fake_datas[4]))[:(le//2)]\n",
    "plt.plot(xf2,r_yf2)\n",
    "print(r_yf2.shape)\n",
    "print(xf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average FFT\n",
    "\n",
    "from scipy.fftpack import fft,ifft\n",
    "r_average_fft = np.zeros(len(real_datas[0]))\n",
    "f_average_fft = np.zeros(len(fake_datas[0]))\n",
    "le = len(real_datas)\n",
    "for i in range(len(real_datas)):\n",
    "    r_average_fft += abs( fft(real_datas[i]) ) / le\n",
    "    f_average_fft += abs( fft(fake_datas[i]) ) / le\n",
    "    \n",
    "ln = len(real_datas[0])\n",
    "import matplotlib.pyplot as plt\n",
    "r_yf2 = r_average_fft[ : (ln//2) ]\n",
    "f_yf2 = f_average_fft[ : (ln//2) ]\n",
    "xf = np.linspace(0,fs,data_length) # 頻率\n",
    "xf2 = xf[:(ln//2)] / 1000\n",
    "\n",
    "#print( r_average_fft.shape)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(xf2,r_yf2,label = 'Real',color=(1,0,0))\n",
    "plt.plot(xf2,f_yf2,label = 'Generate',alpha=0.5)\n",
    "plt.legend(loc='best',fontsize=16)\n",
    "plt.title('FFT of Mixed wave',fontsize=20,color='#F08080')\n",
    "plt.xlabel(\"Frequency(kHz)\",fontsize=16)\n",
    "plt.ylabel(\"Average FFT\",fontsize=16)\n",
    "\n",
    "plt.savefig(save + '/' + \"fft.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( xf2[ np.where(r_yf2>10) ] )\n",
    "print( xf2[ np.where(f_yf2>10) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statisitc - easy version\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def feature_change(chandata):\n",
    "    total_data = []\n",
    "    print(len(chandata))\n",
    "    for i in range(len(chandata)):\n",
    "        print(i)\n",
    "        # statistic data\n",
    "        nobs,(damin,damax),damean,davar,daskew,dakurto = stats.describe( chandata[i] )\n",
    "        total_data.append( [damax, damean, davar**(1/2), daskew, dakurto] )\n",
    "    return np.array( total_data )\n",
    "\n",
    "rdata_statis = feature_change(real_datas)\n",
    "fdata_statis = feature_change(fake_datas)\n",
    "\n",
    "print(rdata_statis.shape)\n",
    "print(fdata_statis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real datas easy version\n",
    "\n",
    "total_mean = np.mean(rdata_statis, axis = 0)\n",
    "total_std = np.std(rdata_statis, axis = 0)\n",
    "total_median = np.median(rdata_statis, axis = 0)\n",
    "\n",
    "print(\"max[v], mean[mv], std[v], skewness, kurtosis\")\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "print(total_median)\n",
    "\n",
    "with open('real_statistic.csv','w',newline=\"\") as csvfile:\n",
    "    outwriter = csv.writer(csvfile)\n",
    "    outwriter.writerow(['feature']+['max[v]', 'mean[mv]', 'std[v]', 'skewness', 'kurtosis'])\n",
    "    outwriter.writerow(['mean']+list(total_mean))\n",
    "    outwriter.writerow(['std']+list(total_std))\n",
    "    outwriter.writerow(['median']+list(total_median))\n",
    "    csvfile.close()\n",
    "    \n",
    "\n",
    "#fake datas\n",
    "\n",
    "total_mean = np.mean(fdata_statis, axis = 0)\n",
    "total_std = np.std(fdata_statis, axis = 0)\n",
    "total_median = np.median(fdata_statis, axis = 0)\n",
    "\n",
    "print(\"max[v], mean[mv],std[v], skewness, kurtosis\")\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "print(total_median)\n",
    "    \n",
    "with open('fake_statistic.csv','w',newline=\"\") as csvfile:\n",
    "    outwriter = csv.writer(csvfile)\n",
    "    outwriter.writerow(['feature']+['max[v]', 'mean[mv]', 'std[v]', 'skewness', 'kurtosis'])\n",
    "    outwriter.writerow(['mean']+list(total_mean))\n",
    "    outwriter.writerow(['std']+list(total_std))\n",
    "    outwriter.writerow(['median']+list(total_median))\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statisitc\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def autocorr(x):\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    return result[result.size//2:]\n",
    "\n",
    "def feature_change(chandata):\n",
    "    total_data = []\n",
    "    print(len(chandata))\n",
    "    for i in range(len(chandata)):\n",
    "        print(i)\n",
    "        # statistic data\n",
    "        nobs,(damin,damax),damean,davar,daskew,dakurto = stats.describe( chandata[i] )\n",
    "        # energy\n",
    "        E_fft = np.sum(chandata[i] ** 2) / len(chandata[i])\n",
    "        # auto statistic data\n",
    "        nobs,(aumin,aumax),aumean,auvar,auskew,aukurto = stats.describe( autocorr(chandata[i]) ) \n",
    "        # peak > 0.05\n",
    "        peaks, _ = find_peaks(chandata[i], height=0.05)\n",
    "        \n",
    "        total_data.append( [damax, damean, E_fft, davar**(1/2), daskew, dakurto, int(len(peaks)), auskew, aukurto] )\n",
    "    return np.array( total_data )\n",
    "\n",
    "rdata_statis = feature_change(real_datas)\n",
    "fdata_statis = feature_change(fake_datas)\n",
    "\n",
    "print(rdata_statis.shape)\n",
    "print(fdata_statis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store random 100 datas to generate CPS and coherence\n",
    "\n",
    "a = save + \"/\" + \"spec_improve_ep200_statisitc\" + \"(cyclo2_fresh)\" + \".csv\"\n",
    "with open(a,'w',newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    for i in range(len(fdata_statis)): \n",
    "        csvWriter.writerow(fdata_statis[i] ) \n",
    "        \n",
    "\n",
    "        \n",
    "b = save + \"/\" + \"real_statistic\" + \"(cyclo2_fresh)\" + \".csv\"\n",
    "with open(b,'w',newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    for i in range(len(rdata_statis)): \n",
    "        csvWriter.writerow(rdata_statis[i] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real datas\n",
    "\n",
    "total_mean = np.mean(rdata_statis, axis = 0)\n",
    "total_std = np.std(rdata_statis, axis = 0)\n",
    "total_median = np.median(rdata_statis, axis = 0)\n",
    "\n",
    "print(\"max[v], mean[v], energy[J], std[v], skewness, kurtosis, peak>0.05, skewauto, kuroauto\")\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "print(total_median)\n",
    "\n",
    "with open(save + '/' + 'real_statistic.csv','w',newline=\"\") as csvfile:\n",
    "    outwriter = csv.writer(csvfile)\n",
    "    outwriter.writerow(['feature']+['max[v]', 'mean[v]', 'energy[J]', 'std[v]', 'skewness', 'kurtosis', 'peak>0.05', 'skewauto', 'kuroauto'])\n",
    "    outwriter.writerow(['mean']+list(total_mean))\n",
    "    outwriter.writerow(['std']+list(total_std))\n",
    "    outwriter.writerow(['median']+list(total_median))\n",
    "    csvfile.close()\n",
    "    \n",
    "\n",
    "#fake datas\n",
    "\n",
    "total_mean = np.mean(fdata_statis, axis = 0)\n",
    "total_std = np.std(fdata_statis, axis = 0)\n",
    "total_median = np.median(fdata_statis, axis = 0)\n",
    "\n",
    "print(\"max[v], mean[v], energy[J], std[v], skewness, kurtosis, peak>0.05, skewauto, kuroauto\")\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "print(total_median)\n",
    "    \n",
    "with open(save + '/' + 'fake_statistic.csv','w',newline=\"\") as csvfile:\n",
    "    outwriter = csv.writer(csvfile)\n",
    "    outwriter.writerow(['feature']+['max[v]', 'mean[v]', 'energy[J]', 'std[v]', 'skewness', 'kurtosis', 'peak>0.05', 'skewauto', 'kuroauto'])\n",
    "    outwriter.writerow(['mean']+list(total_mean))\n",
    "    outwriter.writerow(['std']+list(total_std))\n",
    "    outwriter.writerow(['median']+list(total_median))\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "\n",
    "#r_norm = np.linalg.norm(rdata_statis,axis=0)\n",
    "r_max = np.max(rdata_statis,axis=0)\n",
    "r_min = np.min(rdata_statis, axis = 0)\n",
    "r_mean = np.mean(rdata_statis, axis = 0)\n",
    "rdata_statis_norm = np.zeros(np.shape(rdata_statis))\n",
    "for i in range(rdata_statis_norm.shape[1]):\n",
    "    rdata_statis_norm[:,i] = ( (rdata_statis[:,i]-r_mean[i]) )/(r_max[i]-r_min[i])\n",
    "\n",
    "\n",
    "#f_norm = np.linalg.norm(fdata_statis,axis=0)\n",
    "f_max = np.max(fdata_statis,axis=0)\n",
    "f_min = np.min(fdata_statis, axis = 0)\n",
    "f_mean = np.mean(fdata_statis, axis = 0)\n",
    "fdata_statis_norm = np.zeros(np.shape(fdata_statis))\n",
    "for i in range(fdata_statis_norm.shape[1]):\n",
    "    fdata_statis_norm[:,i] = ( (fdata_statis[:,i]-f_mean[i]) )/(f_max[i]-f_min[i])\n",
    "\n",
    "print(rdata_statis_norm.shape)\n",
    "print(fdata_statis_norm.shape)\n",
    "\n",
    "\n",
    "# mix\n",
    "\n",
    "dataplot = []\n",
    "for i in range(9):\n",
    "    dataplot.append(rdata_statis_norm[:,i])\n",
    "    dataplot.append(fdata_statis_norm[:,i])\n",
    "dataplot = np.transpose(np.array(dataplot))\n",
    "print(dataplot.shape)\n",
    "\n",
    "labels=[]\n",
    "for i in range(1,10):\n",
    "    labels.append(str(i)+\"\")\n",
    "    labels.append('*')\n",
    "\n",
    "fig,axes = plt.subplots()\n",
    "axes.boxplot(x=dataplot,sym='rd',patch_artist=True,labels=labels)\n",
    "axes.set_xlabel('Feature')\n",
    "plt.text(14,0.85,'Num : Real',alpha=1)\n",
    "plt.text(15,0.75,'* : Generate',alpha=1)\n",
    "plt.savefig(save + '/' + \"boxplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
