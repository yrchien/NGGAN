{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 400000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "real_datas = []\n",
    "with open('cyclo2_fresh.csv', newline='') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    for row in rows:\n",
    "        real_datas.append( list( map(float,row) ) )\n",
    "    csvfile.close()\n",
    "real_datas = np.array(real_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "print( stats.describe(real_datas[3]) )\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(np.arange(0,len(real_datas[1])/fs,1/fs),real_datas[10])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zxx (頻率軸,時域軸)\n",
    "# 頻率軸 index = 0 (頻率最高) -> index = last (頻率最低)\n",
    "# 時域軸 index = 0 (時間起點) -> index = last (時間終點)\n",
    "\n",
    "from scipy import signal\n",
    "fs = 400000\n",
    "freq, times, Zxx = signal.stft(real_datas[0], fs=fs, window='blackmanharris', nperseg=256, noverlap=128)\n",
    "print(freq.shape)\n",
    "print(times.shape)\n",
    "print(Zxx.shape)\n",
    "\n",
    "a11 = signal.istft(Zxx, fs=fs, window='blackmanharris', nperseg=256, noverlap=128)\n",
    "print(type(a11[1]))\n",
    "print(\"length={%d}\"%a11[1].shape)\n",
    "\n",
    "freq = freq[:-1]\n",
    "times = times[:-1]\n",
    "Zxx = Zxx[1:,:-1]\n",
    "print(freq.shape)\n",
    "print(times.shape)\n",
    "print(Zxx.shape)\n",
    "\n",
    "# print(freq/1000)\n",
    "# print(times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"raw:\")\n",
    "print(np.max(abs(Zxx)))\n",
    "print(np.min(abs(Zxx)))\n",
    "print(np.mean(abs(Zxx)))\n",
    "\n",
    "k = np.log10(abs(Zxx))\n",
    "\n",
    "print(\"log:\")\n",
    "print(np.max(k))\n",
    "print(np.min(k))\n",
    "print(np.mean(k))\n",
    "\n",
    "print(\"nor:\")\n",
    "tune = 4.2\n",
    "\n",
    "k_ = (k+tune)/tune\n",
    "print(np.max(k_))\n",
    "print(np.min(k_))\n",
    "print(np.mean(k_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune = 4.2\n",
    "for i in range(1,15000,50):\n",
    "    freq, times, Zxx = signal.stft(real_datas[i], fs=fs, window='blackmanharris', nperseg=256, noverlap=128)\n",
    "    k = np.log10(abs(Zxx))\n",
    "    k_ = (k+tune)/tune\n",
    "    print(np.max(k_),np.min(k_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw stft\n",
    "from scipy import signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "cm=plt.cm.get_cmap('plasma')\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.pcolormesh(times, freq/1000, 20*np.log10(abs(Zxx)), cmap=cm)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Frequency(kHz)\",fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training start\n",
    "import Ipynb_importer\n",
    "# %load train.py\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch import optim\n",
    "import json\n",
    "#from utils import save_samples\n",
    "import numpy as np\n",
    "import pprint\n",
    "import pickle\n",
    "import datetime\n",
    "#from dcgan import *\n",
    "from utils import *\n",
    "from logger import *\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============Logger===============\n",
    "LOGGER = logging.getLogger('dcgan')\n",
    "LOGGER.setLevel(logging.DEBUG)\n",
    "\n",
    "LOGGER.info('Initialized logger.')\n",
    "init_console_logger(LOGGER)\n",
    "\n",
    "# =============Parameters===============\n",
    "args = parse_arguments()\n",
    "epochs = args['num_epochs']\n",
    "batch_size = args['batch_size']\n",
    "latent_dim = args['latent_dim']\n",
    "ngpus = args['ngpus']\n",
    "model_size = args['model_size']\n",
    "model_dir = make_path(os.path.join(args['output_dir'],\n",
    "                                   datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")))\n",
    "args['model_dir'] = model_dir\n",
    "\n",
    "# save samples for every N epochs.\n",
    "epochs_per_sample = args['epochs_per_sample']\n",
    "\n",
    "# gradient penalty regularization factor.\n",
    "lmbda = args['lmbda']\n",
    "\n",
    "# Dir\n",
    "audio_dir = args['audio_dir']\n",
    "output_dir = args['output_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test generator\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.utils.data\n",
    "# import numpy as np\n",
    "\n",
    "# nc = 1\n",
    "# nz = 100\n",
    "# ngf = 64\n",
    "# ndf = 64\n",
    "# model_size = 4\n",
    "\n",
    "# # 16 = (8-1)*2 - 2*0 + 1*(4-1) + 0 + 1\n",
    "# conv = nn.ConvTranspose2d( model_size * 16, model_size * 8, 4, 2, 1, bias=False)\n",
    "# conv1 = nn.ConvTranspose2d( model_size * 8, model_size * 4, 4, 2, 1, bias=False)\n",
    "# conv2 = nn.ConvTranspose2d( model_size * 4, model_size * 2, 4, 2, 1, bias=False)\n",
    "# conv3 = nn.ConvTranspose2d( model_size * 2, nc, 3, 1, 1, bias=False) # \n",
    "\n",
    "# a = torch.tensor(np.ones((1,8,8,64)),dtype=torch.float32)\n",
    "# a = a.permute(0,3,1,2)\n",
    "# print(a.shape)\n",
    "# a = conv(a)\n",
    "# print(a.shape)\n",
    "# a = conv1(a)\n",
    "# print(a.shape)\n",
    "# a = conv2(a)\n",
    "# print(a.shape)\n",
    "# a = conv3(a)\n",
    "# print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # test discriminator\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.utils.data\n",
    "# import numpy as np\n",
    "\n",
    "# nc = 1\n",
    "# nz = 100\n",
    "# model_size = 4\n",
    "\n",
    "# # 16 = (8-1)*2 - 2*0 + 1*(4-1) + 0 + 1\n",
    "# conv = nn.Conv2d(nc, model_size*8, 4, 2, 1, bias=False)\n",
    "# leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "# drop = nn.Dropout(p=0.25)\n",
    "# conv1 = nn.Conv2d(model_size*8, model_size*16, 4, 2, 1, bias=False)\n",
    "# conv2 = nn.Conv2d(model_size*16, model_size*32, 4, 2, 1, bias=False)\n",
    "# conv3 = nn.Conv2d(model_size*32, model_size*64, 4, 2, 1, bias=False)\n",
    "\n",
    "# a = torch.tensor(np.ones((2,64,64,1)),dtype=torch.float32)\n",
    "# a = a.permute(0,3,1,2)\n",
    "# print(type(a))\n",
    "# print(a.shape)\n",
    "# a = drop(leaky(conv(a)))\n",
    "# print(a.shape)\n",
    "# a = conv1(a)\n",
    "# print(a.shape)\n",
    "# a = conv2(a)\n",
    "# print(a.shape)\n",
    "# a = conv3(a)\n",
    "# print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Ipynb_importer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 1\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 128\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 128\n",
    "\n",
    "model_size = 64\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu, verbose = False):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.verbose = verbose\n",
    "        self.fc1 = nn.Linear(nz, ndf*ndf) \n",
    "        self.dconv1 = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d( model_size * 16, model_size * 8, 4, 2, 1, bias=True),\n",
    "            nn.BatchNorm2d(model_size * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.dconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d( model_size * 8, model_size * 4, 4, 2, 1, bias=True),\n",
    "            nn.BatchNorm2d(model_size * 4),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.dconv3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(model_size * 4, model_size * 2, 4, 2, 1, bias=True),\n",
    "            nn.BatchNorm2d(model_size * 2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.dconv4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(model_size * 2 , model_size , 4, 2, 1, bias=True),\n",
    "            nn.BatchNorm2d(model_size),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.dconv5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d( model_size , nc, 4, 2, 1, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(self.verbose):\n",
    "            print(x.shape)\n",
    "            \n",
    "        x = self.fc1(x)\n",
    "        x =  x.view(-1,1024,4,4)\n",
    "        if(self.verbose):\n",
    "            print(x.shape)\n",
    "            \n",
    "        x = self.dconv1(x)\n",
    "        if(self.verbose):\n",
    "            print(x.shape)\n",
    "            \n",
    "        x = self.dconv2(x)\n",
    "        if(self.verbose):\n",
    "            print(x.shape)\n",
    "            \n",
    "        x = self.dconv3(x)\n",
    "        if(self.verbose):\n",
    "            print(x.shape)\n",
    "            \n",
    "        x = self.dconv4(x)\n",
    "        if(self.verbose):\n",
    "            print(x.shape)\n",
    "            \n",
    "        x = self.dconv5(x)\n",
    "        if(self.verbose):\n",
    "            print(x.shape)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu, verbose=False):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.verbose = verbose\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(nc, model_size, 4, 2, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(model_size, model_size*2, 4, 2, 1, bias=True),\n",
    "            nn.BatchNorm2d(model_size*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(model_size*2, model_size*4, 4, 2, 1, bias=True),\n",
    "            nn.BatchNorm2d(model_size*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(model_size*4, model_size*8, 4, 2, 1, bias=True),\n",
    "            nn.BatchNorm2d(model_size*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(model_size*8, model_size*16, 4, 2, 1, bias=True),\n",
    "            nn.BatchNorm2d(model_size*16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(p=0.25)\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Linear(16384, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if (self.verbose):\n",
    "            print(x.shape)\n",
    "        x =  self.conv1(x)\n",
    "        if (self.verbose):\n",
    "            print(x.shape)\n",
    "        x =  self.conv2(x)\n",
    "        if (self.verbose):\n",
    "            print(x.shape)\n",
    "        x =  self.conv3(x)\n",
    "        if (self.verbose):\n",
    "            print(x.shape)\n",
    "        x =  self.conv4(x)\n",
    "        if (self.verbose):\n",
    "            print(x.shape)\n",
    "        x =  self.conv5(x)\n",
    "        if (self.verbose):\n",
    "            print(x.shape)\n",
    "        x =  x.view(-1,16384)\n",
    "        if (self.verbose):\n",
    "            print(x.shape)\n",
    "        x =  self.sig( self.fc2( x ) )\n",
    "        if (self.verbose):\n",
    "            print(x.shape)\n",
    "        return  x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test netG\n",
    "ngpu = 1\n",
    "netG = Generator(ngpu,verbose=True)\n",
    "a = torch.rand(1,100)\n",
    "b = netG(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test netD\n",
    "# ngpu = 1\n",
    "# netD = Discriminator(ngpu,verbose=True)\n",
    "# a = torch.rand(1,1,128,128)\n",
    "# b = netD(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "netG = Generator(ngpus)\n",
    "netD = Discriminator(ngpus)\n",
    "\n",
    "if cuda:\n",
    "    netG = torch.nn.DataParallel(netG).cuda()\n",
    "    netD = torch.nn.DataParallel(netD).cuda()\n",
    "\n",
    "# \"Two time-scale update rule\"(TTUR) to update netD 4x faster than netG.\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args['learning_rate'], betas=(args['beta1'], args['beta2']))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=args['learning_rate'], betas=(args['beta1'], args['beta2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample noise used for generated output.\n",
    "sample_noise = torch.randn(args['sample_size'], latent_dim)\n",
    "if cuda:\n",
    "    sample_noise = sample_noise.cuda()\n",
    "sample_noise_Var = autograd.Variable(sample_noise, requires_grad=False)\n",
    "\n",
    "# Save config.\n",
    "LOGGER.info('Saving configurations...')\n",
    "config_path = os.path.join(model_dir, 'config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(args, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER.info('Loading audio data...')\n",
    "\n",
    "# Load noise data #\n",
    "num_ = len(real_datas)\n",
    "num_valid = int(np.ceil(num_ * args['valid_ratio']))\n",
    "num_train = num_ - num_valid\n",
    "train_size = num_train\n",
    "if not (num_valid > 0 and num_train > 0):\n",
    "    LOGGER.error(\"Please download DATASET !\")\n",
    "train_data = batch_generator( real_datas[:num_train],batch_size )\n",
    "valid_data = batch_generator( real_datas[num_train:],batch_size )\n",
    "\n",
    "print(f\"train:size={train_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TRAIN_SAMPLES = train_size\n",
    "BATCH_NUM = TOTAL_TRAIN_SAMPLES // batch_size\n",
    "train_iter = iter(train_data)\n",
    "valid_iter = iter(valid_data)\n",
    "#test_iter = iter(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(train_iter)['X'].shape)\n",
    "print(type(next(train_iter)['X']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_loss = []\n",
    "G_loss = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "LOGGER.info('Starting training...EPOCHS={}, BATCH_SIZE={}, BATCH_NUM={}'.format(epochs, batch_size, BATCH_NUM))\n",
    "for epoch in range(1, epochs+1):\n",
    "    LOGGER.info(\"{} Epoch: {}/{}\".format(time_since(start), epoch, epochs))\n",
    "    \n",
    "    D_loss_epoch = []\n",
    "    G_loss_epoch = []\n",
    "    \n",
    "    for i in range(1, BATCH_NUM+1):\n",
    "        \n",
    "#         # (1) Update D network\n",
    "#         for p in netD.parameters():\n",
    "#             p.requires_grad = True\n",
    "            \n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # latent vector\n",
    "        noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1).view(-1,nz)\n",
    "        if cuda:\n",
    "            noise = noise.cuda()\n",
    "        noise_Var = Variable(noise, requires_grad=False)\n",
    "        \n",
    "        \n",
    "        real_data_Var = numpy_to_var(next(train_iter)['X'], cuda) \n",
    "        b_size = real_data_Var.size(0)\n",
    "\n",
    "        # a) compute loss contribution from real training data\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float)\n",
    "        if cuda:\n",
    "            label = label.cuda()            \n",
    "\n",
    "        D_real = netD(real_data_Var).view(-1)\n",
    "\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(D_real, label)   \n",
    "\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = D_real.mean().item()\n",
    "\n",
    "        # b) compute loss contribution from generated data, then backprop.\n",
    "        fake = autograd.Variable(netG(noise_Var).data)\n",
    "        label.fill_(fake_label)\n",
    "        #print(np.shape(fake))\n",
    "        D_fake = netD(fake).view(-1)\n",
    "\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(D_fake, label)\n",
    "\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward() \n",
    "\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "        \n",
    "        if cuda:\n",
    "            errD = errD.cpu()\n",
    "            \n",
    "        # Record loss \n",
    "        D_loss_epoch.append(errD.detach().numpy())\n",
    "        \n",
    "            \n",
    "            \n",
    "        #######################################################\n",
    "        # (3) Train Generator\n",
    "        #######################################################\n",
    "        # Prevent discriminator update.\n",
    "#         for p in netD.parameters():\n",
    "#             p.requires_grad = False\n",
    "\n",
    "        # Reset generator gradients\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        \n",
    "        # Noise\n",
    "        noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1).view(-1,nz)\n",
    "        if cuda:\n",
    "            noise = noise.cuda()\n",
    "        noise_Var = Variable(noise, requires_grad=False) \n",
    "        \n",
    "        fake = netD(netG(noise_Var)).view(-1)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(fake, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "        \n",
    "        if cuda:\n",
    "            errG = errG.cpu()\n",
    "            \n",
    "        # Record loss \n",
    "        G_loss_epoch.append(errG.detach().numpy())\n",
    "        \n",
    "        LOGGER.info(\"{} Epoch={} Batch: {}/{} D_loss{:.4f} | G_loss:{:.4f}\".format(time_since(start), epoch,\n",
    "                                                                                             i, BATCH_NUM,\n",
    "                                                                                             errD,\n",
    "                                                                                             errG,\n",
    "                                                                                             ))    \n",
    "    \n",
    "    D_loss_epoch_avg = sum(D_loss_epoch) / float(len(D_loss_epoch))\n",
    "    G_loss_epoch_avg = sum(G_loss_epoch) / float(len(G_loss_epoch))\n",
    "    \n",
    "    D_loss.append(D_loss_epoch_avg)\n",
    "    G_loss.append(G_loss_epoch_avg)\n",
    "        \n",
    "    LOGGER.info(\"{} D_loss_epoch:{:.4f} |  G_loss_epoch:{:.4f}\".format(time_since(start),\n",
    "                                       D_loss_epoch_avg,\n",
    "                                       G_loss_epoch_avg,))\n",
    "    \n",
    "\n",
    "    if epoch % epochs_per_sample == 0:\n",
    "        LOGGER.info(\"Generating samples...\")\n",
    "        sample_out = netG(sample_noise_Var.view(-1,100))  \n",
    "        if cuda:\n",
    "            sample_out = sample_out.cpu()\n",
    "        sample_out = sample_out.data.numpy() \n",
    "        sample_out = np.squeeze(sample_out) \n",
    "        # save spectrogram and timeseries and statistic\n",
    "        sample_dir = save_and_plot(sample_out, epoch, output_dir )\n",
    "        \n",
    "    if epoch % 20 ==0:\n",
    "        # Save model\n",
    "        LOGGER.info(\"Saving models...\")\n",
    "        netD_path = os.path.join(output_dir, str(epoch) + \"discriminator.pkl\")\n",
    "        netG_path = os.path.join(output_dir, str(epoch) + \"generator.pkl\")\n",
    "        torch.save(netD.state_dict(), netD_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        torch.save(netG.state_dict(), netG_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "LOGGER.info('>>>>>>>Training finished !<<<<<<<')\n",
    "\n",
    "# Save model\n",
    "LOGGER.info(\"Saving models...\")\n",
    "netD_path = os.path.join(output_dir, \"discriminator.pkl\")\n",
    "netG_path = os.path.join(output_dir, \"generator.pkl\")\n",
    "torch.save(netD.state_dict(), netD_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "torch.save(netG.state_dict(), netG_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# Plot loss curve.\n",
    "LOGGER.info(\"Saving loss curve...\")\n",
    "plot_loss(D_loss, G_loss, output_dir)      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(D_loss, G_loss, save_path):\n",
    "\n",
    "    save_path = os.path.join(save_path, \"loss_curve.png\")\n",
    "\n",
    "    x = range(len(D_loss))\n",
    "\n",
    "    y1 = D_loss\n",
    "    y2 = G_loss\n",
    "\n",
    "    plt.plot(x, y1, label='D_loss')\n",
    "    plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(D_loss, G_loss, output_dir)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成雜訊\n",
    "LOGGER.info(\"Generating samples...\")\n",
    "fakee = []\n",
    "for i in range(470):\n",
    "    noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1).view(-1,100)\n",
    "    if cuda:\n",
    "        noise = noise.cuda()\n",
    "    noise_Var = Variable(noise, requires_grad=False)\n",
    "    sample_out = netG(noise_Var)  \n",
    "    if cuda:\n",
    "        sample_out = sample_out.cpu()\n",
    "    sample_out = sample_out.data.numpy() \n",
    "    sample_out = np.squeeze(sample_out)\n",
    "    fakee.append(sample_out)\n",
    "fakee = np.reshape( np.array(fakee),(-1,128,128) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fakee.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = 'cyclo11024_fake250.csv'\n",
    "# #fakees = []\n",
    "# with open(a,'w',newline='') as csvFile:\n",
    "#     csvWriter = csv.writer(csvFile)\n",
    "#     for i in range(len(fakee)):\n",
    "#         if(i%50==0):\n",
    "#             print(i)\n",
    "#         csvWriter.writerow( spectrogram_to_time(fakee[i])[1] )   #回傳[1]才是訊號 \n",
    "#         #fakees.append( spectrogram_to_time(fakee[i])[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_datas = []\n",
    "for i in range(len(fakee)):\n",
    "    print(i)\n",
    "    fake_datas.append( spectrogram_to_time(fakee[i])[1] )\n",
    "fake_datas = np.array(fake_datas)\n",
    "print(fake_datas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_datas = np.float16(fake_datas[:15000])\n",
    "real_datas = real_datas[:15000]\n",
    "print(fake_datas.shape)\n",
    "print(real_datas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = 'metrice'\n",
    "if not os.path.isdir(save):\n",
    "    os.makedirs(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = save + \"/\" + \"dcgan_ep130\" + \"(cyclo2_data)\" + \".csv\"\n",
    "with open(a,'w',newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    for i in range(len(fake_datas)):\n",
    "        print(i)\n",
    "        csvWriter.writerow( fake_datas[i] ) \n",
    "        \n",
    "\n",
    "        \n",
    "# b = save + \"/\" + \"real\" + \"(cyclo4_data)\" + \".csv\"\n",
    "# with open(b,'w',newline='') as csvFile:\n",
    "#     csvWriter = csv.writer(csvFile)\n",
    "#     for i in x:\n",
    "#         csvWriter.writerow( real_datas[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paremeter\n",
    "fs = 625000 # 400k\n",
    "data_length = len(real_datas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw time series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = data_length / fs # sampling freq * sampling time = sample num\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot( np.arange(0,t,1/fs), real_datas[500], label = 'Real',color = (1,0,0))\n",
    "plt.legend(loc = 'upper right',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Noise amplitude(v)\",fontsize=16)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot( np.arange(0,t,1/fs), fake_datas[2], label = 'Generate')\n",
    "plt.legend(loc = 'upper right',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Noise amplitude(v)\",fontsize=16)\n",
    "\n",
    "\n",
    "plt.savefig(save + '/' + \"time_series.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(1,15000, size=(10))\n",
    "tot = 1\n",
    "# draw random 10 time series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#fs = 360000 # 360k\n",
    "t = data_length / fs # sampling freq * sampling time = sample num\n",
    "\n",
    "plt.figure(figsize=(15,50))\n",
    "for i in x:\n",
    "    plt.subplot(20,1,tot)\n",
    "    plt.plot( np.arange(0,t,1/fs), real_datas[i], label = 'Real',color = (1,0,0))\n",
    "    plt.legend(loc = 'upper right',fontsize=16)\n",
    "    plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "    plt.ylabel(\"Noise amplitude(v)\",fontsize=10)\n",
    "    \n",
    "    tot += 1\n",
    "    \n",
    "    plt.subplot(20,1,tot)\n",
    "    plt.plot( np.arange(0,t,1/fs), fake_datas[i], label = 'Generate')\n",
    "    plt.legend(loc = 'upper right',fontsize=16)\n",
    "    plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "    plt.ylabel(\"Noise amplitude(v)\",fontsize=10)\n",
    "    \n",
    "    tot += 1\n",
    "    \n",
    "plt.savefig(save + '/' + \"time_series(10random).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw stft\n",
    "from scipy import signal\n",
    "# fs = 360000\n",
    "f1, t1, Zxx1 = signal.stft(real_datas[0], fs=fs, window='blackmanharris', nperseg=256, noverlap=128)\n",
    "f2, t2, Zxx2 = signal.stft(fake_datas[0], fs=fs, window='blackmanharris', nperseg=256, noverlap=128)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "cm=plt.cm.get_cmap('plasma')\n",
    "plt.figure(figsize=(20,10))\n",
    "#plt.title(\" X:Time[s], Y:Frequency[KHz]\")\n",
    "plt.subplot(2,1,1)\n",
    "plt.pcolormesh(t1, f1/1000, 20*np.log10(abs(Zxx1)),cmap=cm)\n",
    "plt.colorbar()\n",
    "#plt.legend(loc = 'best',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Frequency(kHz)\",fontsize=16)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.pcolormesh(t2, f2/1000, 20*np.log10(abs(Zxx2)),cmap=cm)\n",
    "plt.colorbar()\n",
    "#plt.legend(loc = 'best',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Frequency(kHz)\",fontsize=16)\n",
    "plt.savefig(save + '/' + \"stft.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft,ifft\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "le = len(real_datas[2])\n",
    "xf = np.linspace(0,fs,data_length) # 頻率\n",
    "xf2 = xf[:(le//2)]\n",
    "r_yf2 = abs(fft(fake_datas[100]))[:(le//2)]\n",
    "plt.plot(xf2,r_yf2)\n",
    "print(r_yf2.shape)\n",
    "print(xf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average FFT\n",
    "\n",
    "from scipy.fftpack import fft,ifft\n",
    "r_average_fft = np.zeros(len(real_datas[0]))\n",
    "f_average_fft = np.zeros(len(fake_datas[0]))\n",
    "le = len(real_datas)\n",
    "for i in range(len(real_datas)):\n",
    "    r_average_fft += 20*np.log10(abs( fft(real_datas[i])/16384 )) / le\n",
    "    f_average_fft += 20*np.log10(abs( fft(fake_datas[i])/16384 )) / le\n",
    "    \n",
    "ln = len(real_datas[0])\n",
    "import matplotlib.pyplot as plt\n",
    "r_yf2 = r_average_fft[ : (ln//2) ]\n",
    "f_yf2 = f_average_fft[ : (ln//2) ]\n",
    "xf = np.linspace(0,fs,data_length) # 頻率\n",
    "xf2 = xf[:(ln//2)] / 1000\n",
    "\n",
    "#print( r_average_fft.shape)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(xf2,r_yf2,label = 'Real',color=(1,0,0))\n",
    "plt.plot(xf2,f_yf2,label = 'Generate',alpha=0.5)\n",
    "plt.legend(loc='best',fontsize=16)\n",
    "plt.title('FFT of Mixed wave',fontsize=20,color='#F08080')\n",
    "plt.xlabel(\"Frequency(kHz)\",fontsize=16)\n",
    "plt.ylabel(\"Average FFT(db)\",fontsize=16)\n",
    "\n",
    "plt.savefig(save + '/' + \"fft.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statisitc\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def autocorr(x):\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    return result[result.size//2:]\n",
    "\n",
    "def feature_change(chandata):\n",
    "    total_data = []\n",
    "    print(len(chandata))\n",
    "    for i in range(len(chandata)):\n",
    "        print(i)\n",
    "        # statistic data\n",
    "        nobs,(damin,damax),damean,davar,daskew,dakurto = stats.describe( chandata[i] )\n",
    "        # energy\n",
    "        E_fft = np.sum(chandata[i] ** 2) / len(chandata[i])\n",
    "        # auto statistic data\n",
    "        nobs,(aumin,aumax),aumean,auvar,auskew,aukurto = stats.describe( autocorr(chandata[i]) ) \n",
    "        # peak > 0.05\n",
    "        peaks, _ = find_peaks(chandata[i], height=0.05)\n",
    "        \n",
    "        total_data.append( [damax, damean*1000, E_fft, davar**(1/2), daskew, dakurto, int(len(peaks)), auskew, aukurto] )\n",
    "    return np.array( total_data )\n",
    "\n",
    "rdata_statis = feature_change(real_datas)\n",
    "fdata_statis = feature_change(fake_datas)\n",
    "\n",
    "print(rdata_statis.shape)\n",
    "print(fdata_statis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store random 100 datas to generate CPS and coherence\n",
    "\n",
    "a = save + \"/\" + \"dcgan_ep130_statistic\" + \"(cyclo4_data)\" + \".csv\"\n",
    "with open(a,'w',newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    for i in range(len(fdata_statis)):\n",
    "        csvWriter.writerow( fdata_statis[i] ) \n",
    "        \n",
    "\n",
    "        \n",
    "b = save + \"/\" + \"real_statistic\" + \"(cyclo4_data)\" + \".csv\"\n",
    "with open(b,'w',newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    for i in range(len(rdata_statis)):\n",
    "        csvWriter.writerow( rdata_statis[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real datas\n",
    "\n",
    "total_mean = np.mean(rdata_statis, axis = 0)\n",
    "total_std = np.std(rdata_statis, axis = 0)\n",
    "total_median = np.median(rdata_statis, axis = 0)\n",
    "\n",
    "print(\"max[v], mean[mv], energy[J], std[v], skewness, kurtosis, peak>0.05, skewauto, kuroauto\")\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "print(total_median)\n",
    "\n",
    "with open(save + '/' + 'real_statistic.csv','w',newline=\"\") as csvfile:\n",
    "    outwriter = csv.writer(csvfile)\n",
    "    outwriter.writerow(['feature']+['max[v]', 'mean[mv]', 'energy[J]', 'std[v]', 'skewness', 'kurtosis', 'peak>0.05', 'skewauto', 'kuroauto'])\n",
    "    outwriter.writerow(['mean']+list(total_mean))\n",
    "    outwriter.writerow(['std']+list(total_std))\n",
    "    outwriter.writerow(['median']+list(total_median))\n",
    "    csvfile.close()\n",
    "    \n",
    "\n",
    "#fake datas\n",
    "\n",
    "total_mean = np.mean(fdata_statis, axis = 0)\n",
    "total_std = np.std(fdata_statis, axis = 0)\n",
    "total_median = np.median(fdata_statis, axis = 0)\n",
    "\n",
    "print(\"max[v], mean[mv], energy[J], std[v], skewness, kurtosis, peak>0.05, skewauto, kuroauto\")\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "print(total_median)\n",
    "    \n",
    "with open(save + '/' + 'fake_statistic.csv','w',newline=\"\") as csvfile:\n",
    "    outwriter = csv.writer(csvfile)\n",
    "    outwriter.writerow(['feature']+['max[v]', 'mean[mv]', 'energy[J]', 'std[v]', 'skewness', 'kurtosis', 'peak>0.05', 'skewauto', 'kuroauto'])\n",
    "    outwriter.writerow(['mean']+list(total_mean))\n",
    "    outwriter.writerow(['std']+list(total_std))\n",
    "    outwriter.writerow(['median']+list(total_median))\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "\n",
    "#r_norm = np.linalg.norm(rdata_statis,axis=0)\n",
    "r_max = np.max(rdata_statis,axis=0)\n",
    "r_min = np.min(rdata_statis, axis = 0)\n",
    "r_mean = np.mean(rdata_statis, axis = 0)\n",
    "rdata_statis_norm = np.zeros(np.shape(rdata_statis))\n",
    "for i in range(rdata_statis_norm.shape[1]):\n",
    "    rdata_statis_norm[:,i] = ( (rdata_statis[:,i]-r_mean[i]) )/(r_max[i]-r_min[i])\n",
    "\n",
    "\n",
    "#f_norm = np.linalg.norm(fdata_statis,axis=0)\n",
    "f_max = np.max(fdata_statis,axis=0)\n",
    "f_min = np.min(fdata_statis, axis = 0)\n",
    "f_mean = np.mean(fdata_statis, axis = 0)\n",
    "fdata_statis_norm = np.zeros(np.shape(fdata_statis))\n",
    "for i in range(fdata_statis_norm.shape[1]):\n",
    "    fdata_statis_norm[:,i] = ( (fdata_statis[:,i]-f_mean[i]) )/(f_max[i]-f_min[i])\n",
    "\n",
    "print(rdata_statis_norm.shape)\n",
    "print(fdata_statis_norm.shape)\n",
    "\n",
    "\n",
    "# mix\n",
    "\n",
    "dataplot = []\n",
    "for i in range(9):\n",
    "    dataplot.append(rdata_statis_norm[:,i])\n",
    "    dataplot.append(fdata_statis_norm[:,i])\n",
    "dataplot = np.transpose(np.array(dataplot))\n",
    "print(dataplot.shape)\n",
    "\n",
    "labels=[]\n",
    "for i in range(1,10):\n",
    "    labels.append(str(i)+\"\")\n",
    "    labels.append('*')\n",
    "\n",
    "fig,axes = plt.subplots()\n",
    "axes.boxplot(x=dataplot,sym='rd',patch_artist=True,labels=labels)\n",
    "axes.set_xlabel('Feature')\n",
    "plt.text(14,0.90,'Num : Real')\n",
    "plt.text(15,0.80,'* : Generate')\n",
    "plt.savefig(save + '/' + \"boxplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "\n",
    "#r_norm = np.linalg.norm(rdata_statis,axis=0)\n",
    "r_max = np.max(rdata_statis,axis=0)\n",
    "r_min = np.min(rdata_statis, axis = 0)\n",
    "r_mean = np.mean(rdata_statis, axis = 0)\n",
    "rdata_statis_norm = np.zeros(np.shape(rdata_statis))\n",
    "for i in range(rdata_statis_norm.shape[1]):\n",
    "    maxx = abs(r_max[i]) if abs(r_max[i])>abs(r_min[i]) else abs(r_min[i])\n",
    "    rdata_statis_norm[:,i] =  rdata_statis[:,i] /maxx\n",
    "\n",
    "\n",
    "#f_norm = np.linalg.norm(fdata_statis,axis=0)\n",
    "f_max = np.max(fdata_statis,axis=0)\n",
    "f_min = np.min(fdata_statis, axis = 0)\n",
    "f_mean = np.mean(fdata_statis, axis = 0)\n",
    "fdata_statis_norm = np.zeros(np.shape(fdata_statis))\n",
    "for i in range(fdata_statis_norm.shape[1]):\n",
    "    maxx = abs(f_max[i]) if abs(f_max[i])>abs(f_min[i]) else abs(f_min[i])\n",
    "    fdata_statis_norm[:,i] =  fdata_statis[:,i] /maxx\n",
    "print(rdata_statis_norm.shape)\n",
    "print(fdata_statis_norm.shape)\n",
    "\n",
    "\n",
    "# mix\n",
    "\n",
    "dataplot = []\n",
    "for i in range(9):\n",
    "    dataplot.append(rdata_statis_norm[:,i])\n",
    "    dataplot.append(fdata_statis_norm[:,i])\n",
    "dataplot = np.transpose(np.array(dataplot))\n",
    "print(dataplot.shape)\n",
    "\n",
    "labels=[]\n",
    "for i in range(1,10):\n",
    "    labels.append(str(i)+\"\")\n",
    "    labels.append('*')\n",
    "\n",
    "fig,axes = plt.subplots()\n",
    "axes.boxplot(x=dataplot,sym='rd',patch_artist=True,labels=labels)\n",
    "axes.set_xlabel('Feature')\n",
    "plt.text(14,-0.8,'Num : Real')\n",
    "plt.text(15,-0.9,'* : Generate')\n",
    "plt.savefig(save + '/' + \"boxplot_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r_min)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
