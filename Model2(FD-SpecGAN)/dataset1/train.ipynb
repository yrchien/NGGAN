{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 調整參數\n",
    "fs = 400000 #400k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cyclos data\n",
    "import csv\n",
    "import numpy as np\n",
    "real_datas = []\n",
    "with open('cyclo2_fresh.csv', newline='') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    for row in rows:\n",
    "        real_datas.append( list( map(float,row) ) )\n",
    "    csvfile.close()\n",
    "real_datas = np.array(real_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "print( stats.describe(real_datas[0]) )\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(np.arange(0,len(real_datas[1])/fs,1/fs),real_datas[10])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency amplitude in (-1,1) downside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "#fs = 360000\n",
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "cm=plt.cm.get_cmap('rainbow')\n",
    "freq, times, Zxx = signal.stft(real_datas[0], fs=fs, nperseg=257, noverlap=128)\n",
    "print(Zxx.shape)\n",
    "# freq = freq[1:]\n",
    "# Zxx = Zxx[1:]\n",
    "\n",
    "# # handle abs \n",
    "# lownum = 1e-8\n",
    "# Zxx2 = abs(Zxx) + lownum\n",
    "# Zxx2log = np.log10(Zxx2)\n",
    "# minva = -4\n",
    "# Zxx2nor = ( Zxx2log - (minva) )/abs(minva)\n",
    "# plt.pcolormesh(times, freq, Zxx2nor,cmap=cm)\n",
    "\n",
    "# # handle the angle\n",
    "# phase = np.angle(Zxx)\n",
    "# finetuneangle = 3.15\n",
    "# tophase = phase / finetuneangle\n",
    "\n",
    "\n",
    "# print(np.max(Zxx2nor),np.min(Zxx2nor),np.mean(Zxx2nor))\n",
    "# print(np.max(tophase),np.min(tophase),np.mean(tophase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "fs = 360000\n",
    "for i in range(100):\n",
    "    freq, times, Zxx = signal.stft(datas[i], fs=fs, nperseg=257, noverlap=128)\n",
    "    freq = freq[1:]\n",
    "    Zxx = Zxx[1:]\n",
    "    # print(freq.shape)\n",
    "    # print(times.shape)\n",
    "    # print(Zxx.shape)\n",
    "\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    # print(np.mean(abs(Zxx)))\n",
    "    # print(np.max(abs(Zxx)))\n",
    "    # print(np.min(abs(Zxx)))\n",
    "\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    fs = 360000\n",
    "    lownum = 1e-8\n",
    "    Zxx2 = abs(Zxx) + lownum\n",
    "    Zxx2log = np.log10(Zxx2)\n",
    "    # print(np.mean(Zxx2log))\n",
    "    # print(np.max(Zxx2log))\n",
    "    # print(np.min(Zxx2log))\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    minva = -4\n",
    "    Zxx2nor = ( Zxx2log - (minva) )/abs(minva)\n",
    "    print(np.mean(Zxx2nor))\n",
    "    print(np.max(Zxx2nor))\n",
    "    print(np.min(Zxx2nor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from specgan.ipynb\n",
      "importing Jupyter notebook from utils.ipynb\n",
      "importing Jupyter notebook from config.ipynb\n",
      "importing Jupyter notebook from logger.ipynb\n"
     ]
    }
   ],
   "source": [
    "# training start\n",
    "import Ipynb_importer\n",
    "# %load train.py\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch import optim\n",
    "import json\n",
    "#from utils import save_samples\n",
    "import numpy as np\n",
    "import pprint\n",
    "import pickle\n",
    "import datetime\n",
    "from specgan import *\n",
    "from utils import *\n",
    "from logger import *\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============Logger===============\n",
    "LOGGER = logging.getLogger('wavegan')\n",
    "LOGGER.setLevel(logging.DEBUG)\n",
    "\n",
    "LOGGER.info('Initialized logger.')\n",
    "init_console_logger(LOGGER)\n",
    "\n",
    "# =============Parameters===============\n",
    "args = parse_arguments()\n",
    "epochs = args['num_epochs']\n",
    "batch_size = args['batch_size']\n",
    "latent_dim = args['latent_dim']\n",
    "ngpus = args['ngpus']\n",
    "model_size = args['model_size']\n",
    "model_dir = make_path(os.path.join(args['output_dir'],\n",
    "                                   datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")))\n",
    "args['model_dir'] = model_dir\n",
    "# save samples for every N epochs.\n",
    "epochs_per_sample = args['epochs_per_sample']\n",
    "# gradient penalty regularization factor.\n",
    "lmbda = args['lmbda']\n",
    "\n",
    "# Dir\n",
    "audio_dir = args['audio_dir']\n",
    "output_dir = args['output_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = SpecGANGenerator(model_size=model_size, ngpus=ngpus, latent_dim=latent_dim, upsample=True)\n",
    "netD = SpecGANDiscriminator(model_size=model_size, ngpus=ngpus)\n",
    "\n",
    "\n",
    "if cuda:\n",
    "    netG = torch.nn.DataParallel(netG).cuda()\n",
    "    netD = torch.nn.DataParallel(netD).cuda()\n",
    "\n",
    "# if pretrained \n",
    "# netD_path = os.path.join(output_dir, \"200discriminator.pkl\")\n",
    "# netG_path = os.path.join(output_dir, \"200generator.pkl\")\n",
    "\n",
    "# netG.load_state_dict(torch.load(netG_path))\n",
    "# netD.load_state_dict(torch.load(netD_path))\n",
    "\n",
    "# \"Two time-scale update rule\"(TTUR) to update netD 4x faster than netG.\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args['learning_rate'], betas=(args['beta1'], args['beta2']))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=args['learning_rate'], betas=(args['beta1'], args['beta2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Saving configurations...\n"
     ]
    }
   ],
   "source": [
    "# Sample noise used for generated output.\n",
    "sample_noise = torch.randn(args['sample_size'], latent_dim)\n",
    "if cuda:\n",
    "    sample_noise = sample_noise.cuda()\n",
    "sample_noise_Var = autograd.Variable(sample_noise, requires_grad=False)\n",
    "\n",
    "# Save config.\n",
    "LOGGER.info('Saving configurations...')\n",
    "config_path = os.path.join(model_dir, 'config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(args, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading audio data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Loading audio data...\n"
     ]
    }
   ],
   "source": [
    "LOGGER.info('Loading audio data...')\n",
    "\n",
    "# Load cs09 data #\n",
    "# audio_paths = get_all_audio_filepaths(audio_dir) #list\n",
    "# train_data, valid_data, test_data, train_size = split_data(audio_paths, args['valid_ratio'],\n",
    "#                                                            args['test_ratio'], batch_size)\n",
    "\n",
    "# Load noise data #\n",
    "num_ = len(real_datas)\n",
    "num_valid = int(np.ceil(num_ * args['valid_ratio']))\n",
    "num_train = num_ - num_valid\n",
    "train_size = num_train\n",
    "if not (num_valid > 0 and num_train > 0):\n",
    "    LOGGER.error(\"Please download DATASET !\")\n",
    "train_data = batch_generator( real_datas[:num_train],batch_size )\n",
    "valid_data = batch_generator( real_datas[num_train:],batch_size )\n",
    "\n",
    "TOTAL_TRAIN_SAMPLES = train_size\n",
    "BATCH_NUM = TOTAL_TRAIN_SAMPLES // batch_size\n",
    "train_iter = iter(train_data)\n",
    "valid_iter = iter(valid_data)\n",
    "#test_iter = iter(test_data)\n",
    "\n",
    "history = []\n",
    "D_costs_train = []\n",
    "D_wasses_train = []\n",
    "D_costs_valid = []\n",
    "D_wasses_valid = []\n",
    "G_costs = []\n",
    "\n",
    "pkl = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Starting training...EPOCHS=200, BATCH_SIZE=64, BATCH_NUM=220\n",
      "[INFO] 0m 0s Epoch: 1/200\n",
      "[INFO] 0m 2s Epoch=1 Batch: 1/220 D_c:-2506.4004 | D_w:2506.5493 | G:1252.5160\n",
      "[INFO] 0m 5s Epoch=1 Batch: 2/220 D_c:-2518.9312 | D_w:2519.3267 | G:1259.2800\n",
      "[INFO] 0m 8s Epoch=1 Batch: 3/220 D_c:-2532.0815 | D_w:2532.5776 | G:1265.5997\n",
      "[INFO] 0m 11s Epoch=1 Batch: 4/220 D_c:-2543.0740 | D_w:2543.3296 | G:1271.8052\n",
      "[INFO] 0m 14s Epoch=1 Batch: 5/220 D_c:-2558.5403 | D_w:2558.9036 | G:1278.9902\n",
      "[INFO] 0m 17s Epoch=1 Batch: 6/220 D_c:-2569.4050 | D_w:2569.6396 | G:1285.2880\n",
      "[INFO] 0m 20s Epoch=1 Batch: 7/220 D_c:-2584.2527 | D_w:2584.5742 | G:1291.8766\n",
      "[INFO] 0m 23s Epoch=1 Batch: 8/220 D_c:-2597.5647 | D_w:2598.0347 | G:1298.5381\n",
      "[INFO] 0m 26s Epoch=1 Batch: 9/220 D_c:-2608.0103 | D_w:2608.7666 | G:1304.0209\n",
      "[INFO] 0m 29s Epoch=1 Batch: 10/220 D_c:-2623.5872 | D_w:2623.8281 | G:1311.7992\n",
      "[INFO] 0m 32s Epoch=1 Batch: 11/220 D_c:-2638.3049 | D_w:2638.4622 | G:1318.8602\n",
      "[INFO] 0m 35s Epoch=1 Batch: 12/220 D_c:-2650.6526 | D_w:2650.8467 | G:1325.5569\n",
      "[INFO] 0m 38s Epoch=1 Batch: 13/220 D_c:-2664.4399 | D_w:2664.5918 | G:1332.0190\n",
      "[INFO] 0m 41s Epoch=1 Batch: 14/220 D_c:-2674.9763 | D_w:2675.8271 | G:1335.4767\n",
      "[INFO] 0m 44s Epoch=1 Batch: 15/220 D_c:-2690.7539 | D_w:2691.1348 | G:1345.3787\n",
      "[INFO] 0m 47s Epoch=1 Batch: 16/220 D_c:-2704.1116 | D_w:2704.2927 | G:1352.3555\n",
      "[INFO] 0m 50s Epoch=1 Batch: 17/220 D_c:-2717.4502 | D_w:2717.9131 | G:1358.6515\n",
      "[INFO] 0m 52s Epoch=1 Batch: 18/220 D_c:-2730.7068 | D_w:2730.8657 | G:1365.0696\n",
      "[INFO] 0m 55s Epoch=1 Batch: 19/220 D_c:-2743.9609 | D_w:2744.3252 | G:1372.1299\n",
      "[INFO] 0m 58s Epoch=1 Batch: 20/220 D_c:-2756.1892 | D_w:2756.4585 | G:1378.2864\n",
      "[INFO] 1m 1s Epoch=1 Batch: 21/220 D_c:-2771.1831 | D_w:2771.3174 | G:1385.5243\n",
      "[INFO] 1m 4s Epoch=1 Batch: 22/220 D_c:-2784.9128 | D_w:2785.1050 | G:1392.2579\n",
      "[INFO] 1m 7s Epoch=1 Batch: 23/220 D_c:-2797.8069 | D_w:2797.9224 | G:1398.8906\n",
      "[INFO] 1m 10s Epoch=1 Batch: 24/220 D_c:-2811.3779 | D_w:2811.6440 | G:1405.7954\n",
      "[INFO] 1m 13s Epoch=1 Batch: 25/220 D_c:-2824.2429 | D_w:2824.6641 | G:1411.3845\n",
      "[INFO] 1m 16s Epoch=1 Batch: 26/220 D_c:-2837.4675 | D_w:2837.8259 | G:1418.9156\n",
      "[INFO] 1m 19s Epoch=1 Batch: 27/220 D_c:-2851.9600 | D_w:2852.2109 | G:1426.1672\n",
      "[INFO] 1m 22s Epoch=1 Batch: 28/220 D_c:-2865.7314 | D_w:2865.8120 | G:1432.9398\n",
      "[INFO] 1m 25s Epoch=1 Batch: 29/220 D_c:-2877.7747 | D_w:2877.9985 | G:1439.4349\n",
      "[INFO] 1m 28s Epoch=1 Batch: 30/220 D_c:-2892.4072 | D_w:2892.7129 | G:1446.3744\n",
      "[INFO] 1m 31s Epoch=1 Batch: 31/220 D_c:-2905.5354 | D_w:2905.6257 | G:1453.0883\n",
      "[INFO] 1m 34s Epoch=1 Batch: 32/220 D_c:-2919.6589 | D_w:2919.7622 | G:1460.1074\n",
      "[INFO] 1m 37s Epoch=1 Batch: 33/220 D_c:-2933.0425 | D_w:2933.3896 | G:1464.9353\n",
      "[INFO] 1m 40s Epoch=1 Batch: 34/220 D_c:-2945.1992 | D_w:2946.5247 | G:1472.6283\n",
      "[INFO] 1m 43s Epoch=1 Batch: 35/220 D_c:-2960.8557 | D_w:2961.1001 | G:1480.5636\n",
      "[INFO] 1m 46s Epoch=1 Batch: 36/220 D_c:-2973.6807 | D_w:2973.8882 | G:1487.6040\n",
      "[INFO] 1m 48s Epoch=1 Batch: 37/220 D_c:-2987.2708 | D_w:2987.7732 | G:1493.8855\n",
      "[INFO] 1m 51s Epoch=1 Batch: 38/220 D_c:-3001.8064 | D_w:3001.8479 | G:1501.1704\n",
      "[INFO] 1m 54s Epoch=1 Batch: 39/220 D_c:-3015.3550 | D_w:3015.4114 | G:1508.1517\n",
      "[INFO] 1m 57s Epoch=1 Batch: 40/220 D_c:-3029.0386 | D_w:3029.2480 | G:1514.9756\n",
      "[INFO] 2m 0s Epoch=1 Batch: 41/220 D_c:-3042.3489 | D_w:3042.4844 | G:1521.6807\n",
      "[INFO] 2m 3s Epoch=1 Batch: 42/220 D_c:-3055.8652 | D_w:3056.1843 | G:1528.3698\n",
      "[INFO] 2m 6s Epoch=1 Batch: 43/220 D_c:-3068.5383 | D_w:3069.5723 | G:1534.9846\n",
      "[INFO] 2m 9s Epoch=1 Batch: 44/220 D_c:-3083.3005 | D_w:3083.7915 | G:1542.0669\n",
      "[INFO] 2m 12s Epoch=1 Batch: 45/220 D_c:-3097.1079 | D_w:3097.4563 | G:1549.0868\n",
      "[INFO] 2m 15s Epoch=1 Batch: 46/220 D_c:-3111.0393 | D_w:3111.3882 | G:1556.1893\n",
      "[INFO] 2m 18s Epoch=1 Batch: 47/220 D_c:-3124.9451 | D_w:3125.1128 | G:1562.8918\n",
      "[INFO] 2m 21s Epoch=1 Batch: 48/220 D_c:-3137.5225 | D_w:3138.4097 | G:1567.2578\n",
      "[INFO] 2m 24s Epoch=1 Batch: 49/220 D_c:-3151.8450 | D_w:3152.9949 | G:1576.2223\n",
      "[INFO] 2m 27s Epoch=1 Batch: 50/220 D_c:-3166.7466 | D_w:3167.1787 | G:1583.9083\n",
      "[INFO] 2m 30s Epoch=1 Batch: 51/220 D_c:-3180.4331 | D_w:3180.5771 | G:1591.1963\n",
      "[INFO] 2m 33s Epoch=1 Batch: 52/220 D_c:-3194.0640 | D_w:3194.1924 | G:1597.4833\n",
      "[INFO] 2m 36s Epoch=1 Batch: 53/220 D_c:-3207.4294 | D_w:3207.5669 | G:1604.7161\n",
      "[INFO] 2m 39s Epoch=1 Batch: 54/220 D_c:-3221.8511 | D_w:3221.9778 | G:1611.3718\n",
      "[INFO] 2m 42s Epoch=1 Batch: 55/220 D_c:-3236.2378 | D_w:3236.4895 | G:1619.0291\n",
      "[INFO] 2m 45s Epoch=1 Batch: 56/220 D_c:-3250.2505 | D_w:3250.4954 | G:1625.7307\n",
      "[INFO] 2m 48s Epoch=1 Batch: 57/220 D_c:-3262.8604 | D_w:3263.3184 | G:1631.7155\n",
      "[INFO] 2m 51s Epoch=1 Batch: 58/220 D_c:-3278.0332 | D_w:3278.2158 | G:1639.8309\n",
      "[INFO] 2m 54s Epoch=1 Batch: 59/220 D_c:-3290.7898 | D_w:3290.9375 | G:1646.5896\n",
      "[INFO] 2m 57s Epoch=1 Batch: 60/220 D_c:-3306.2947 | D_w:3306.3726 | G:1653.9709\n",
      "[INFO] 3m 0s Epoch=1 Batch: 61/220 D_c:-3315.0576 | D_w:3317.6726 | G:1657.0107\n",
      "[INFO] 3m 3s Epoch=1 Batch: 62/220 D_c:-3332.6667 | D_w:3332.9302 | G:1667.8179\n",
      "[INFO] 3m 6s Epoch=1 Batch: 63/220 D_c:-3349.1626 | D_w:3349.2432 | G:1675.3688\n",
      "[INFO] 3m 9s Epoch=1 Batch: 64/220 D_c:-3362.7573 | D_w:3362.8154 | G:1682.4250\n",
      "[INFO] 3m 12s Epoch=1 Batch: 65/220 D_c:-3376.4233 | D_w:3376.5679 | G:1689.3088\n",
      "[INFO] 3m 15s Epoch=1 Batch: 66/220 D_c:-3391.2866 | D_w:3391.3704 | G:1696.5406\n",
      "[INFO] 3m 18s Epoch=1 Batch: 67/220 D_c:-3404.6765 | D_w:3405.0815 | G:1702.0762\n",
      "[INFO] 3m 20s Epoch=1 Batch: 68/220 D_c:-3417.5076 | D_w:3418.1665 | G:1709.8832\n",
      "[INFO] 3m 23s Epoch=1 Batch: 69/220 D_c:-3434.0698 | D_w:3434.1484 | G:1718.0717\n",
      "[INFO] 3m 26s Epoch=1 Batch: 70/220 D_c:-3447.7114 | D_w:3448.0764 | G:1724.1914\n",
      "[INFO] 3m 29s Epoch=1 Batch: 71/220 D_c:-3461.5247 | D_w:3461.8940 | G:1732.2607\n",
      "[INFO] 3m 32s Epoch=1 Batch: 72/220 D_c:-3476.6589 | D_w:3476.8433 | G:1739.4332\n",
      "[INFO] 3m 35s Epoch=1 Batch: 73/220 D_c:-3489.9238 | D_w:3490.1802 | G:1746.2015\n",
      "[INFO] 3m 38s Epoch=1 Batch: 74/220 D_c:-3504.7478 | D_w:3504.8423 | G:1753.7567\n",
      "[INFO] 3m 41s Epoch=1 Batch: 75/220 D_c:-3517.5422 | D_w:3518.2268 | G:1760.4812\n",
      "[INFO] 3m 44s Epoch=1 Batch: 76/220 D_c:-3533.3970 | D_w:3533.4941 | G:1767.9268\n",
      "[INFO] 3m 47s Epoch=1 Batch: 77/220 D_c:-3547.3367 | D_w:3547.5933 | G:1775.0878\n",
      "[INFO] 3m 50s Epoch=1 Batch: 78/220 D_c:-3560.7002 | D_w:3561.5286 | G:1783.4911\n",
      "[INFO] 3m 53s Epoch=1 Batch: 79/220 D_c:-3580.4514 | D_w:3580.5950 | G:1792.8668\n",
      "[INFO] 3m 56s Epoch=1 Batch: 80/220 D_c:-3595.2700 | D_w:3595.8745 | G:1800.4570\n",
      "[INFO] 3m 59s Epoch=1 Batch: 81/220 D_c:-3609.6855 | D_w:3609.8706 | G:1808.1150\n",
      "[INFO] 4m 2s Epoch=1 Batch: 82/220 D_c:-3624.6382 | D_w:3625.0820 | G:1815.1121\n",
      "[INFO] 4m 5s Epoch=1 Batch: 83/220 D_c:-3638.5935 | D_w:3639.1587 | G:1822.5433\n",
      "[INFO] 4m 8s Epoch=1 Batch: 84/220 D_c:-3653.5710 | D_w:3653.8860 | G:1830.2034\n",
      "[INFO] 4m 11s Epoch=1 Batch: 85/220 D_c:-3668.1477 | D_w:3668.5598 | G:1837.2324\n",
      "[INFO] 4m 14s Epoch=1 Batch: 86/220 D_c:-3679.6211 | D_w:3679.9336 | G:1844.2229\n",
      "[INFO] 4m 17s Epoch=1 Batch: 87/220 D_c:-3697.3652 | D_w:3697.5649 | G:1851.8871\n",
      "[INFO] 4m 20s Epoch=1 Batch: 88/220 D_c:-3711.7336 | D_w:3711.9785 | G:1859.2562\n",
      "[INFO] 4m 23s Epoch=1 Batch: 89/220 D_c:-3725.3958 | D_w:3725.8735 | G:1866.1304\n",
      "[INFO] 4m 26s Epoch=1 Batch: 90/220 D_c:-3737.6160 | D_w:3738.0667 | G:1872.6753\n",
      "[INFO] 4m 29s Epoch=1 Batch: 91/220 D_c:-3756.0486 | D_w:3756.1775 | G:1881.4493\n",
      "[INFO] 4m 32s Epoch=1 Batch: 92/220 D_c:-3771.0042 | D_w:3771.1768 | G:1888.9700\n",
      "[INFO] 4m 35s Epoch=1 Batch: 93/220 D_c:-3784.1335 | D_w:3784.7185 | G:1896.2012\n",
      "[INFO] 4m 38s Epoch=1 Batch: 94/220 D_c:-3799.5806 | D_w:3799.8550 | G:1903.3271\n",
      "[INFO] 4m 41s Epoch=1 Batch: 95/220 D_c:-3814.4570 | D_w:3814.6328 | G:1910.9685\n",
      "[INFO] 4m 44s Epoch=1 Batch: 96/220 D_c:-3828.6453 | D_w:3828.8013 | G:1918.0508\n",
      "[INFO] 4m 47s Epoch=1 Batch: 97/220 D_c:-3843.0608 | D_w:3843.5405 | G:1925.4182\n",
      "[INFO] 4m 49s Epoch=1 Batch: 98/220 D_c:-3856.1646 | D_w:3858.2004 | G:1929.5156\n",
      "[INFO] 4m 52s Epoch=1 Batch: 99/220 D_c:-3872.3118 | D_w:3872.6951 | G:1940.0508\n",
      "[INFO] 4m 55s Epoch=1 Batch: 100/220 D_c:-3866.0076 | D_w:3871.1538 | G:1945.6311\n",
      "[INFO] 4m 58s Epoch=1 Batch: 101/220 D_c:-3903.2415 | D_w:3904.2683 | G:1955.3789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 5m 1s Epoch=1 Batch: 102/220 D_c:-3921.3442 | D_w:3921.4875 | G:1963.6973\n",
      "[INFO] 5m 4s Epoch=1 Batch: 103/220 D_c:-3936.4109 | D_w:3936.5823 | G:1971.0387\n",
      "[INFO] 5m 7s Epoch=1 Batch: 104/220 D_c:-3949.2078 | D_w:3950.0293 | G:1976.4196\n",
      "[INFO] 5m 10s Epoch=1 Batch: 105/220 D_c:-3965.0132 | D_w:3965.2400 | G:1985.5021\n",
      "[INFO] 5m 13s Epoch=1 Batch: 106/220 D_c:-3980.6873 | D_w:3980.8125 | G:1993.4407\n",
      "[INFO] 5m 16s Epoch=1 Batch: 107/220 D_c:-3994.2869 | D_w:3995.2080 | G:1999.4357\n",
      "[INFO] 5m 19s Epoch=1 Batch: 108/220 D_c:-4010.2566 | D_w:4010.6245 | G:2008.2893\n",
      "[INFO] 5m 22s Epoch=1 Batch: 109/220 D_c:-4024.2400 | D_w:4024.7109 | G:2015.3965\n",
      "[INFO] 5m 25s Epoch=1 Batch: 110/220 D_c:-4040.0762 | D_w:4040.8022 | G:2023.3635\n",
      "[INFO] 5m 28s Epoch=1 Batch: 111/220 D_c:-4053.8845 | D_w:4054.4414 | G:2028.2991\n",
      "[INFO] 5m 31s Epoch=1 Batch: 112/220 D_c:-4067.2810 | D_w:4067.8677 | G:2037.9238\n",
      "[INFO] 5m 34s Epoch=1 Batch: 113/220 D_c:-4087.1147 | D_w:4087.4097 | G:2047.4490\n",
      "[INFO] 5m 37s Epoch=1 Batch: 114/220 D_c:-4102.4102 | D_w:4103.0259 | G:2054.9585\n",
      "[INFO] 5m 40s Epoch=1 Batch: 115/220 D_c:-4113.9976 | D_w:4114.1313 | G:2062.2241\n",
      "[INFO] 5m 43s Epoch=1 Batch: 116/220 D_c:-4132.1782 | D_w:4132.6982 | G:2070.4331\n",
      "[INFO] 5m 46s Epoch=1 Batch: 117/220 D_c:-4147.7432 | D_w:4148.0459 | G:2078.0459\n",
      "[INFO] 5m 49s Epoch=1 Batch: 118/220 D_c:-4157.8467 | D_w:4158.0381 | G:2084.9861\n",
      "[INFO] 5m 52s Epoch=1 Batch: 119/220 D_c:-4177.2310 | D_w:4177.5244 | G:2092.6917\n",
      "[INFO] 5m 55s Epoch=1 Batch: 120/220 D_c:-4190.3394 | D_w:4190.6602 | G:2099.5229\n",
      "[INFO] 5m 58s Epoch=1 Batch: 121/220 D_c:-4207.5801 | D_w:4207.9058 | G:2107.7061\n",
      "[INFO] 6m 0s Epoch=1 Batch: 122/220 D_c:-4222.3013 | D_w:4222.9453 | G:2115.0391\n",
      "[INFO] 6m 3s Epoch=1 Batch: 123/220 D_c:-4235.9458 | D_w:4237.1367 | G:2119.5156\n",
      "[INFO] 6m 6s Epoch=1 Batch: 124/220 D_c:-4250.5776 | D_w:4250.8979 | G:2130.3135\n",
      "[INFO] 6m 9s Epoch=1 Batch: 125/220 D_c:-4268.6660 | D_w:4269.4282 | G:2139.3115\n",
      "[INFO] 6m 12s Epoch=1 Batch: 126/220 D_c:-4284.5874 | D_w:4285.1289 | G:2147.0747\n",
      "[INFO] 6m 15s Epoch=1 Batch: 127/220 D_c:-4299.1152 | D_w:4299.4292 | G:2154.9365\n",
      "[INFO] 6m 18s Epoch=1 Batch: 128/220 D_c:-4314.9805 | D_w:4315.2715 | G:2162.5317\n",
      "[INFO] 6m 21s Epoch=1 Batch: 129/220 D_c:-4329.4131 | D_w:4330.0586 | G:2169.8848\n",
      "[INFO] 6m 24s Epoch=1 Batch: 130/220 D_c:-4344.9893 | D_w:4345.3115 | G:2177.7134\n",
      "[INFO] 6m 27s Epoch=1 Batch: 131/220 D_c:-4360.0972 | D_w:4360.3809 | G:2185.3230\n",
      "[INFO] 6m 30s Epoch=1 Batch: 132/220 D_c:-4375.0508 | D_w:4375.4351 | G:2193.1777\n",
      "[INFO] 6m 33s Epoch=1 Batch: 133/220 D_c:-4391.3950 | D_w:4391.5513 | G:2200.9922\n",
      "[INFO] 6m 36s Epoch=1 Batch: 134/220 D_c:-4406.9131 | D_w:4407.0469 | G:2208.8879\n",
      "[INFO] 6m 39s Epoch=1 Batch: 135/220 D_c:-4421.6538 | D_w:4421.9238 | G:2216.4956\n",
      "[INFO] 6m 42s Epoch=1 Batch: 136/220 D_c:-4436.4956 | D_w:4437.5908 | G:2223.8462\n",
      "[INFO] 6m 45s Epoch=1 Batch: 137/220 D_c:-4450.8374 | D_w:4451.1157 | G:2231.4033\n",
      "[INFO] 6m 48s Epoch=1 Batch: 138/220 D_c:-4467.3945 | D_w:4468.2949 | G:2238.9465\n",
      "[INFO] 6m 51s Epoch=1 Batch: 139/220 D_c:-4482.9756 | D_w:4483.5225 | G:2246.9175\n",
      "[INFO] 6m 53s Epoch=1 Batch: 140/220 D_c:-4497.2720 | D_w:4497.5356 | G:2254.5200\n",
      "[INFO] 6m 56s Epoch=1 Batch: 141/220 D_c:-4512.9507 | D_w:4513.3984 | G:2262.2095\n",
      "[INFO] 6m 59s Epoch=1 Batch: 142/220 D_c:-4529.8633 | D_w:4530.2305 | G:2270.6968\n",
      "[INFO] 7m 2s Epoch=1 Batch: 143/220 D_c:-4544.5000 | D_w:4544.9155 | G:2277.9819\n",
      "[INFO] 7m 5s Epoch=1 Batch: 144/220 D_c:-4560.1230 | D_w:4560.4385 | G:2286.0400\n",
      "[INFO] 7m 8s Epoch=1 Batch: 145/220 D_c:-4575.6382 | D_w:4575.7832 | G:2294.0298\n",
      "[INFO] 7m 11s Epoch=1 Batch: 146/220 D_c:-4591.7041 | D_w:4591.8472 | G:2301.8601\n",
      "[INFO] 7m 14s Epoch=1 Batch: 147/220 D_c:-4606.3052 | D_w:4606.4443 | G:2308.3989\n",
      "[INFO] 7m 17s Epoch=1 Batch: 148/220 D_c:-4620.4995 | D_w:4621.6323 | G:2316.5151\n",
      "[INFO] 7m 20s Epoch=1 Batch: 149/220 D_c:-4637.6411 | D_w:4638.0615 | G:2324.9883\n",
      "[INFO] 7m 23s Epoch=1 Batch: 150/220 D_c:-4653.1235 | D_w:4653.3174 | G:2332.9734\n",
      "[INFO] 7m 26s Epoch=1 Batch: 151/220 D_c:-4668.5190 | D_w:4668.6445 | G:2340.6279\n",
      "[INFO] 7m 29s Epoch=1 Batch: 152/220 D_c:-4684.8813 | D_w:4684.9478 | G:2348.7249\n",
      "[INFO] 7m 32s Epoch=1 Batch: 153/220 D_c:-4699.6357 | D_w:4700.3340 | G:2354.7158\n",
      "[INFO] 7m 35s Epoch=1 Batch: 154/220 D_c:-4715.3979 | D_w:4715.6377 | G:2364.0068\n",
      "[INFO] 7m 38s Epoch=1 Batch: 155/220 D_c:-4730.4697 | D_w:4731.7539 | G:2371.0928\n",
      "[INFO] 7m 41s Epoch=1 Batch: 156/220 D_c:-4747.0518 | D_w:4747.3604 | G:2379.8823\n",
      "[INFO] 7m 44s Epoch=1 Batch: 157/220 D_c:-4762.9956 | D_w:4763.2432 | G:2387.9663\n",
      "[INFO] 7m 47s Epoch=1 Batch: 158/220 D_c:-4778.4536 | D_w:4778.6670 | G:2395.9136\n",
      "[INFO] 7m 50s Epoch=1 Batch: 159/220 D_c:-4793.5972 | D_w:4793.7744 | G:2403.3350\n",
      "[INFO] 7m 53s Epoch=1 Batch: 160/220 D_c:-4808.9292 | D_w:4809.3838 | G:2411.4575\n",
      "[INFO] 7m 56s Epoch=1 Batch: 161/220 D_c:-4825.9604 | D_w:4826.0972 | G:2419.7529\n",
      "[INFO] 7m 59s Epoch=1 Batch: 162/220 D_c:-4841.4805 | D_w:4841.6318 | G:2427.6416\n",
      "[INFO] 8m 2s Epoch=1 Batch: 163/220 D_c:-4856.8076 | D_w:4857.0107 | G:2435.4478\n",
      "[INFO] 8m 5s Epoch=1 Batch: 164/220 D_c:-4871.4468 | D_w:4871.6104 | G:2443.2988\n",
      "[INFO] 8m 7s Epoch=1 Batch: 165/220 D_c:-4888.4712 | D_w:4888.6748 | G:2450.9468\n",
      "[INFO] 8m 10s Epoch=1 Batch: 166/220 D_c:-4904.6714 | D_w:4904.7700 | G:2459.2166\n",
      "[INFO] 8m 13s Epoch=1 Batch: 167/220 D_c:-4920.5386 | D_w:4920.6685 | G:2467.1143\n",
      "[INFO] 8m 16s Epoch=1 Batch: 168/220 D_c:-4935.7412 | D_w:4936.0942 | G:2474.2173\n",
      "[INFO] 8m 19s Epoch=1 Batch: 169/220 D_c:-4951.5625 | D_w:4951.7305 | G:2483.0374\n",
      "[INFO] 8m 22s Epoch=1 Batch: 170/220 D_c:-4967.7700 | D_w:4968.0400 | G:2490.2207\n"
     ]
    }
   ],
   "source": [
    "#epochs = 300\n",
    "pre_epoch =0\n",
    "start = time.time()\n",
    "LOGGER.info('Starting training...EPOCHS={}, BATCH_SIZE={}, BATCH_NUM={}'.format(epochs, batch_size, BATCH_NUM))\n",
    "for epoch in range(1+pre_epoch, epochs+1):\n",
    "    LOGGER.info(\"{} Epoch: {}/{}\".format(time_since(start), epoch, epochs))\n",
    "\n",
    "    D_cost_train_epoch = []\n",
    "    D_wass_train_epoch = []\n",
    "    D_cost_valid_epoch = []\n",
    "    D_wass_valid_epoch = []\n",
    "    G_cost_epoch = []\n",
    "    for i in range(1, BATCH_NUM+1):\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        one = torch.tensor(1, dtype=torch.float)\n",
    "        neg_one = one * -1\n",
    "        if cuda:\n",
    "            one = one.cuda()\n",
    "            neg_one = neg_one.cuda()\n",
    "\n",
    "        #######################################################\n",
    "        # (3) Train Discriminator\n",
    "        #######################################################\n",
    "        for iter_dis in range(5): # 5D 1G\n",
    "            netD.zero_grad()\n",
    "            noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
    "            if cuda:\n",
    "                noise = noise.cuda()\n",
    "            noise_Var = Variable(noise, requires_grad=False)\n",
    "            real_data_Var = numpy_to_var(next(train_iter)['X'], cuda) \n",
    "\n",
    "            # a) compute loss contribution from real training data\n",
    "            D_real = netD(real_data_Var)\n",
    "            D_real = D_real.mean()  # avg loss \n",
    "            D_real.backward(neg_one)  # loss * -1    \n",
    "            \n",
    "            # b) compute loss contribution from generated data, then backprop.\n",
    "            fake = autograd.Variable(netG(noise_Var).data)\n",
    "            #print(np.shape(fake))\n",
    "            D_fake = netD(fake)\n",
    "            D_fake = D_fake.mean()\n",
    "            D_fake.backward(one)\n",
    "            \n",
    "            # c) compute gradient penalty and backprop\n",
    "            gradient_penalty = calc_gradient_penalty(netD, real_data_Var.data,\n",
    "                                                     fake.data, batch_size, lmbda,\n",
    "                                                     use_cuda=cuda)\n",
    "            gradient_penalty.backward(one)\n",
    "\n",
    "            # Compute cost * Wassertein loss..\n",
    "            D_cost_train = D_fake - D_real + gradient_penalty\n",
    "            D_wass_train = D_real - D_fake \n",
    "            \n",
    "            # Update gradient of discriminator.\n",
    "            optimizerD.step()\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            #############################\n",
    "            # (2) Compute Valid data\n",
    "            #############################\n",
    "            netD.zero_grad()\n",
    "\n",
    "            valid_data_Var = numpy_to_var(next(valid_iter)['X'], cuda)\n",
    "            D_real_valid = netD(valid_data_Var)\n",
    "            D_real_valid = D_real_valid.mean()  # avg loss\n",
    "\n",
    "            # b) compute loss contribution from generated data, then backprop.\n",
    "            fake_valid = netG(noise_Var)\n",
    "            D_fake_valid = netD(fake_valid)\n",
    "            D_fake_valid = D_fake_valid.mean()\n",
    "\n",
    "            # c) compute gradient penalty and backprop\n",
    "            gradient_penalty_valid = calc_gradient_penalty(netD, valid_data_Var.data,\n",
    "                                                           fake_valid.data, batch_size, lmbda,\n",
    "                                                           use_cuda=cuda)\n",
    "            # Compute metrics and record in batch history.\n",
    "            D_cost_valid = D_fake_valid - D_real_valid + gradient_penalty_valid\n",
    "            D_wass_valid = D_real_valid - D_fake_valid\n",
    "\n",
    "            if cuda:\n",
    "                D_cost_train = D_cost_train.cpu()\n",
    "                D_wass_train = D_wass_train.cpu()\n",
    "                D_cost_valid = D_cost_valid.cpu()\n",
    "                D_wass_valid = D_wass_valid.cpu()\n",
    "\n",
    "            # Record costs\n",
    "            D_cost_train_epoch.append(D_cost_train.data.numpy())\n",
    "            D_wass_train_epoch.append(D_wass_train.data.numpy())\n",
    "            D_cost_valid_epoch.append(D_cost_valid.data.numpy())\n",
    "            D_wass_valid_epoch.append(D_wass_valid.data.numpy())\n",
    "            \n",
    "\n",
    "        #######################################################\n",
    "        # (3) Train Generator\n",
    "        #######################################################\n",
    "        # Prevent discriminator update.\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Reset generator gradients\n",
    "        netG.zero_grad()\n",
    "\n",
    "        # Noise\n",
    "        noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
    "        if cuda:\n",
    "            noise = noise.cuda()\n",
    "        noise_Var = Variable(noise, requires_grad=False) \n",
    "\n",
    "        fake = netG(noise_Var)\n",
    "        G = netD(fake)\n",
    "        G = G.mean()\n",
    "\n",
    "        # Update gradients.\n",
    "        G.backward(neg_one)\n",
    "        G_cost = -G\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Record costs\n",
    "        if cuda:\n",
    "            G_cost = G_cost.cpu()\n",
    "        G_cost_epoch.append(G_cost.data.numpy())\n",
    "\n",
    "        LOGGER.info(\"{} Epoch={} Batch: {}/{} D_c:{:.4f} | D_w:{:.4f} | G:{:.4f}\".format(time_since(start), epoch,\n",
    "                                                                                             i, BATCH_NUM,\n",
    "                                                                                             D_cost_train.data.numpy(),\n",
    "                                                                                             D_wass_train.data.numpy(),\n",
    "                                                                                             G_cost.data.numpy()))\n",
    "\n",
    "\n",
    "    # Save the average cost of batches in every epoch.\n",
    "    D_cost_train_epoch_avg = sum(D_cost_train_epoch) / float(len(D_cost_train_epoch))\n",
    "    D_wass_train_epoch_avg = sum(D_wass_train_epoch) / float(len(D_wass_train_epoch))\n",
    "    D_cost_valid_epoch_avg = sum(D_cost_valid_epoch) / float(len(D_cost_valid_epoch))\n",
    "    D_wass_valid_epoch_avg = sum(D_wass_valid_epoch) / float(len(D_wass_valid_epoch))\n",
    "    G_cost_epoch_avg = sum(G_cost_epoch) / float(len(G_cost_epoch))\n",
    "\n",
    "    D_costs_train.append(D_cost_train_epoch_avg)\n",
    "    D_wasses_train.append(D_wass_train_epoch_avg)\n",
    "    D_costs_valid.append(D_cost_valid_epoch_avg)\n",
    "    D_wasses_valid.append(D_wass_valid_epoch_avg)\n",
    "    G_costs.append(G_cost_epoch_avg)\n",
    "    \n",
    "\n",
    "    LOGGER.info(\"{} D_cost_train:{:.4f} | D_wass_train:{:.4f} | D_cost_valid:{:.4f} | D_wass_valid:{:.4f} | \"\n",
    "                \"G_cost:{:.4f}\".format(time_since(start),\n",
    "                                       D_cost_train_epoch_avg,\n",
    "                                       D_wass_train_epoch_avg,\n",
    "                                       D_cost_valid_epoch_avg,\n",
    "                                       D_wass_valid_epoch_avg,\n",
    "                                       G_cost_epoch_avg))\n",
    "    \n",
    "\n",
    "    #Generate audio samples.\n",
    "#     if epoch % epochs_per_sample == 0:\n",
    "#         LOGGER.info(\"Generating samples...\")\n",
    "#         sample_out = netG(sample_noise_Var)  # sample_noise_Var = (10,100)\n",
    "#         if cuda:\n",
    "#             sample_out = sample_out.cpu()\n",
    "#         sample_out = sample_out.data.numpy() # sample_out = (10,1,128,128)\n",
    "#         sample_out = np.squeeze(sample_out) \n",
    "#         # save spectrogram and timeseries and statistic\n",
    "#         sample_dir = save_and_plot( times, freq, sample_out, epoch, output_dir )\n",
    "        \n",
    "    if epoch % 20 ==0:\n",
    "        # Save model\n",
    "        LOGGER.info(\"Saving models...\")\n",
    "        netD_path = os.path.join(output_dir, str(epoch) + \"discriminator.pkl\")\n",
    "        netG_path = os.path.join(output_dir, str(epoch) + \"generator.pkl\")\n",
    "        torch.save(netD.state_dict(), netD_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        torch.save(netG.state_dict(), netG_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pkl+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "LOGGER.info('>>>>>>>Training finished !<<<<<<<')\n",
    "\n",
    "# Save model\n",
    "LOGGER.info(\"Saving models...\")\n",
    "netD_path = os.path.join(output_dir, \"discriminator.pkl\")\n",
    "netG_path = os.path.join(output_dir, \"generator.pkl\")\n",
    "torch.save(netD.state_dict(), netD_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "torch.save(netG.state_dict(), netG_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Plot loss curve.\n",
    "LOGGER.info(\"Saving loss curve...\")\n",
    "plot_loss(D_costs_train, D_wasses_train,\n",
    "          D_costs_valid, D_wasses_valid, G_costs, output_dir)\n",
    "\n",
    "LOGGER.info(\"All finished!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate cyclo noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER.info(\"Generating samples...\")\n",
    "fakee = []\n",
    "for i in range(245):\n",
    "    noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
    "    if cuda:\n",
    "        noise = noise.cuda()\n",
    "    noise_Var = Variable(noise, requires_grad=False)\n",
    "    sample_out = netG(noise_Var)  \n",
    "    if cuda:\n",
    "        sample_out = sample_out.cpu()\n",
    "    sample_out = sample_out.data.numpy() # sample_out = (10,1,128,128)\n",
    "    sample_out = np.squeeze(sample_out)\n",
    "    fakee.append(sample_out)\n",
    "fakee = np.reshape( np.array(fakee),(-1,128,128) )\n",
    "print(fakee.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fakee.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### MIX GAN ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cyclcos data\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "Wavedatas = []\n",
    "dataname = 'w_nearest_ep200(cyclo2_fresh)'\n",
    "with open( dataname +'.csv', newline='') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    for row in rows:\n",
    "        Wavedatas.append( list( map(float,row) ) )\n",
    "    csvfile.close()\n",
    "Wavedatas = np.array(Wavedatas)\n",
    "print(Wavedatas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, t1, Zxx1 = signal.stft(Wavedatas[i], fs=400000, nperseg=257, noverlap=128)\n",
    "print(Zxx1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = 400000\n",
    "g_iters = 15\n",
    "\n",
    "def _stft_(y):\n",
    "    f, t, Zxx = signal.stft(y, fs=freqs, nperseg=257, noverlap=128)\n",
    "    return Zxx\n",
    "\n",
    "def _griffin_lim(S,ang):\n",
    "    m = np.mean(S,axis=0)\n",
    "    S = np.vstack((m,S))\n",
    "    angles = ang\n",
    "    S_complex = np.abs(S).astype(np.complex)\n",
    "    \n",
    "#     for i in range(g_iters):\n",
    "#         if i > 0:\n",
    "#             angles = np.exp(1j * np.angle(_stft_(y[1])))\n",
    "#         y = _istft_( S_complex * np.exp(1j*angles) )\n",
    "    y = _istft_( S_complex * np.exp(1j*angles) )\n",
    "    return y[1]\n",
    "\n",
    "def _istft_(S):\n",
    "    return signal.istft(S, fs=freqs, nperseg=257, noverlap=128)\n",
    "\n",
    "def spec_to_time(spectrogram,angle):\n",
    "    minva = -4\n",
    "    lownum = 1e-8\n",
    "    Zxx2denor = spectrogram*abs(minva) + minva\n",
    "    Zxx2dbtoamp = np.power(10,Zxx2denor)- lownum\n",
    "    estimate = _griffin_lim(Zxx2dbtoamp, ang)\n",
    "    return estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, t1, Zxx1 = signal.stft(Wavedatas[1], fs=400000, nperseg=257, noverlap=128)\n",
    "ang = np.angle(Zxx1)\n",
    "a = spec_to_time(fakee[1], ang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_datas = []\n",
    "for i in range(len(fakee)):\n",
    "    print(i)\n",
    "    f1, t1, Zxx1 = signal.stft(Wavedatas[i], fs=400000, nperseg=257, noverlap=128)\n",
    "    ang = np.angle(Zxx1)\n",
    "    fake_datas.append( spec_to_time(fakee[i], ang) )\n",
    "fake_datas = np.array(fake_datas)\n",
    "print(fake_datas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = 'metrice'\n",
    "if not os.path.isdir(save):\n",
    "    os.makedirs(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = save + \"/\" + \"mixgan_nogriffin_ep200\" + \"(cyclo2_fresh)\" + \".csv\"\n",
    "with open(a,'w',newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    for i in range(len(fake_datas)): \n",
    "        csvWriter.writerow( fake_datas[i] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ end #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrogram\n",
    "cm=plt.cm.get_cmap('rainbow') \n",
    "plt.figure(figsize=(30,10))\n",
    "plt.pcolormesh(fakee[0],cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_series\n",
    "plt.plot(spectrogram_to_time(fakee[0])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fake_datas = []\n",
    "for i in range(len(fakee)):\n",
    "    print(i)\n",
    "    fake_datas.append( spectrogram_to_time(fakee[i])[1] )\n",
    "fake_datas = np.array(fake_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### validate ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_datas = []\n",
    "l = len(fakee)\n",
    "for i in range(len(fakee)):\n",
    "    print(i)\n",
    "    fake_datas.append( spectrogram_to_time(fakee[i])[1] )\n",
    "fake_datas = np.array(fake_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "#real_datas = np.array(real_datas[:15000])\n",
    "fake_datas = np.array(fake_datas[:15000])\n",
    "#print(real_datas.shape)\n",
    "print(fake_datas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new folder to store metrice\n",
    "\n",
    "save = 'metrice'\n",
    "if not os.path.isdir(save):\n",
    "    os.makedirs(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = save + \"/\" + \"spec_ep200\" + \"(cyclo2_fresh)\" + \".csv\"\n",
    "with open(a,'w',newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    for i in range(len(fake_datas)): \n",
    "        csvWriter.writerow( fake_datas[i] ) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paremeter\n",
    "fs = 400000 # 400k\n",
    "data_length = len(real_datas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw time series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#fs = 360000 # 360k\n",
    "t = data_length / fs # sampling freq * sampling time = sample num\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot( np.arange(0,t,1/fs), real_datas[500], label = 'Real',color = (1,0,0))\n",
    "plt.legend(loc = 'upper right',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Noise amplitude(v)\",fontsize=16)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot( np.arange(0,t,1/fs), fake_datas[2], label = 'Generate')\n",
    "plt.legend(loc = 'upper right',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Noise amplitude(v)\",fontsize=16)\n",
    "\n",
    "\n",
    "plt.savefig(save + '/' + \"time_series.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(1,15000, size=(10))\n",
    "tot = 1\n",
    "# draw random 10 time series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#fs = 360000 # 360k\n",
    "t = data_length / fs # sampling freq * sampling time = sample num\n",
    "\n",
    "plt.figure(figsize=(15,50))\n",
    "for i in x:\n",
    "    plt.subplot(20,1,tot)\n",
    "    plt.plot( np.arange(0,t,1/fs), real_datas[i], label = 'Real',color = (1,0,0))\n",
    "    plt.legend(loc = 'upper right',fontsize=16)\n",
    "    plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "    plt.ylabel(\"Noise amplitude(v)\",fontsize=10)\n",
    "    \n",
    "    tot += 1\n",
    "    \n",
    "    plt.subplot(20,1,tot)\n",
    "    plt.plot( np.arange(0,t,1/fs), fake_datas[i], label = 'Generate')\n",
    "    plt.legend(loc = 'upper right',fontsize=16)\n",
    "    plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "    plt.ylabel(\"Noise amplitude(v)\",fontsize=10)\n",
    "    \n",
    "    tot += 1\n",
    "    \n",
    "plt.savefig(save + '/' + \"time_series(10random).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw stft\n",
    "from scipy import signal\n",
    "# fs = 360000\n",
    "f1, t1, Zxx1 = signal.stft(real_datas[500], fs=fs, nperseg=257, noverlap=128)\n",
    "f2, t2, Zxx2 = signal.stft(fake_datas[1], fs=fs, nperseg=257, noverlap=128)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "cm=plt.cm.get_cmap('plasma')\n",
    "plt.figure(figsize=(20,10))\n",
    "#plt.title(\" X:Time[s], Y:Frequency[KHz]\")\n",
    "plt.subplot(2,1,1)\n",
    "plt.pcolormesh(t1, f1/1000, abs(Zxx1),cmap=cm)\n",
    "plt.colorbar()\n",
    "#plt.legend(loc = 'best',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Frequency(kHz)\",fontsize=16)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.pcolormesh(t2, f2/1000, abs(Zxx2),cmap=cm)\n",
    "plt.colorbar()\n",
    "#plt.legend(loc = 'best',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Frequency(kHz)\",fontsize=16)\n",
    "plt.savefig(save + '/' + \"stft.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency value\n",
    "print(np.where(Zxx1>0.20))\n",
    "print(f1[43]/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft,ifft\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "le = len(real_datas[2])\n",
    "xf = np.linspace(0,fs,data_length) # 頻率\n",
    "xf2 = xf[:(le//2)]\n",
    "r_yf2 = abs(fft(fake_datas[3]))[:(le//2)]\n",
    "plt.plot(xf2,r_yf2)\n",
    "print(r_yf2.shape)\n",
    "print(xf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average FFT\n",
    "\n",
    "from scipy.fftpack import fft,ifft\n",
    "r_average_fft = np.zeros(len(real_datas[0]))\n",
    "f_average_fft = np.zeros(len(fake_datas[0]))\n",
    "le = len(real_datas)\n",
    "for i in range(len(real_datas)):\n",
    "    r_average_fft += abs( fft(real_datas[i]) ) / le\n",
    "    f_average_fft += abs( fft(fake_datas[i]) ) / le\n",
    "    \n",
    "ln = len(real_datas[0])\n",
    "import matplotlib.pyplot as plt\n",
    "r_yf2 = r_average_fft[ : (ln//2) ]\n",
    "f_yf2 = f_average_fft[ : (ln//2) ]\n",
    "xf = np.linspace(0,fs,data_length) # 頻率\n",
    "xf2 = xf[:(ln//2)] / 1000\n",
    "\n",
    "#print( r_average_fft.shape)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(xf2,r_yf2,label = 'Real',color=(1,0,0))\n",
    "plt.plot(xf2,f_yf2,label = 'Generate',alpha=0.5)\n",
    "plt.legend(loc='best',fontsize=16)\n",
    "plt.title('FFT of Mixed wave',fontsize=20,color='#F08080')\n",
    "plt.xlabel(\"Frequency(kHz)\",fontsize=16)\n",
    "plt.ylabel(\"Average FFT\",fontsize=16)\n",
    "\n",
    "plt.savefig(save + '/' + \"fft.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( np.where(f_yf2>200) )\n",
    "print( xf[2707]/1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statisitc - easy version\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def feature_change(chandata):\n",
    "    total_data = []\n",
    "    print(len(chandata))\n",
    "    for i in range(len(chandata)):\n",
    "        print(i)\n",
    "        # statistic data\n",
    "        nobs,(damin,damax),damean,davar,daskew,dakurto = stats.describe( chandata[i] )\n",
    "        total_data.append( [damax, damean, davar**(1/2), daskew, dakurto] )\n",
    "    return np.array( total_data )\n",
    "\n",
    "rdata_statis = feature_change(real_datas)\n",
    "fdata_statis = feature_change(fake_datas)\n",
    "\n",
    "fdata_statis = fdata_statis[:15675]\n",
    "print(rdata_statis.shape)\n",
    "print(fdata_statis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real datas easy version\n",
    "\n",
    "total_mean = np.mean(rdata_statis, axis = 0)\n",
    "total_std = np.std(rdata_statis, axis = 0)\n",
    "total_median = np.median(rdata_statis, axis = 0)\n",
    "\n",
    "print(\"max[v], mean[mv], std[v], skewness, kurtosis\")\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "print(total_median)\n",
    "\n",
    "with open('real_statistic.csv','w',newline=\"\") as csvfile:\n",
    "    outwriter = csv.writer(csvfile)\n",
    "    outwriter.writerow(['feature']+['max[v]', 'mean[mv]', 'std[v]', 'skewness', 'kurtosis'])\n",
    "    outwriter.writerow(['mean']+list(total_mean))\n",
    "    outwriter.writerow(['std']+list(total_std))\n",
    "    outwriter.writerow(['median']+list(total_median))\n",
    "    csvfile.close()\n",
    "    \n",
    "\n",
    "#fake datas\n",
    "\n",
    "total_mean = np.mean(fdata_statis, axis = 0)\n",
    "total_std = np.std(fdata_statis, axis = 0)\n",
    "total_median = np.median(fdata_statis, axis = 0)\n",
    "\n",
    "print(\"max[v], mean[mv],std[v], skewness, kurtosis\")\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "print(total_median)\n",
    "    \n",
    "with open('fake_statistic.csv','w',newline=\"\") as csvfile:\n",
    "    outwriter = csv.writer(csvfile)\n",
    "    outwriter.writerow(['feature']+['max[v]', 'mean[mv]', 'std[v]', 'skewness', 'kurtosis'])\n",
    "    outwriter.writerow(['mean']+list(total_mean))\n",
    "    outwriter.writerow(['std']+list(total_std))\n",
    "    outwriter.writerow(['median']+list(total_median))\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statisitc\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def autocorr(x):\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    return result[result.size//2:]\n",
    "\n",
    "def feature_change(chandata):\n",
    "    total_data = []\n",
    "    print(len(chandata))\n",
    "    for i in range(len(chandata)):\n",
    "        print(i)\n",
    "        # statistic data\n",
    "        nobs,(damin,damax),damean,davar,daskew,dakurto = stats.describe( chandata[i] )\n",
    "        # energy\n",
    "        E_fft = np.sum(chandata[i] ** 2) / len(chandata[i])\n",
    "        # auto statistic data\n",
    "        nobs,(aumin,aumax),aumean,auvar,auskew,aukurto = stats.describe( autocorr(chandata[i]) ) \n",
    "        # peak > 0.05\n",
    "        peaks, _ = find_peaks(chandata[i], height=0.05)\n",
    "        \n",
    "        total_data.append( [damax, damean, E_fft, davar**(1/2), daskew, dakurto, int(len(peaks)), auskew, aukurto] )\n",
    "    return np.array( total_data )\n",
    "\n",
    "rdata_statis = feature_change(real_datas)\n",
    "fdata_statis = feature_change(fake_datas)\n",
    "\n",
    "print(rdata_statis.shape)\n",
    "print(fdata_statis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdata_statis.shape)\n",
    "print(fdata_statis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store random 100 datas to generate CPS and coherence\n",
    "\n",
    "a = save + \"/\" + \"spec_ep200_statistic\" + \"(cyclo2_fresh)\" + \".csv\"\n",
    "with open(a,'w',newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    for i in range(len(fdata_statis)):\n",
    "        csvWriter.writerow( fdata_statis[i] ) \n",
    "        \n",
    "\n",
    "        \n",
    "b = save + \"/\" + \"real_statistic\" + \"(cyclo2_fresh)\" + \".csv\"\n",
    "with open(b,'w',newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    for i in range(len(rdata_statis)):\n",
    "        csvWriter.writerow( rdata_statis[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real datas\n",
    "\n",
    "total_mean = np.mean(rdata_statis, axis = 0)\n",
    "total_std = np.std(rdata_statis, axis = 0)\n",
    "total_median = np.median(rdata_statis, axis = 0)\n",
    "\n",
    "print(\"max[v], mean[v], energy[J], std[v], skewness, kurtosis, peak>0.05, skewauto, kuroauto\")\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "print(total_median)\n",
    "\n",
    "with open(save + '/' + 'real_statistic.csv','w',newline=\"\") as csvfile:\n",
    "    outwriter = csv.writer(csvfile)\n",
    "    outwriter.writerow(['feature']+['max[v]', 'mean[v]', 'energy[J]', 'std[v]', 'skewness', 'kurtosis', 'peak>0.05', 'skewauto', 'kuroauto'])\n",
    "    outwriter.writerow(['mean']+list(total_mean))\n",
    "    outwriter.writerow(['std']+list(total_std))\n",
    "    outwriter.writerow(['median']+list(total_median))\n",
    "    csvfile.close()\n",
    "    \n",
    "\n",
    "#fake datas\n",
    "\n",
    "total_mean = np.mean(fdata_statis, axis = 0)\n",
    "total_std = np.std(fdata_statis, axis = 0)\n",
    "total_median = np.median(fdata_statis, axis = 0)\n",
    "\n",
    "print(\"max[v], mean[v], energy[J], std[v], skewness, kurtosis, peak>0.05, skewauto, kuroauto\")\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "print(total_median)\n",
    "    \n",
    "with open(save + '/' + 'fake_statistic.csv','w',newline=\"\") as csvfile:\n",
    "    outwriter = csv.writer(csvfile)\n",
    "    outwriter.writerow(['feature']+['max[v]', 'mean[v]', 'energy[J]', 'std[v]', 'skewness', 'kurtosis', 'peak>0.05', 'skewauto', 'kuroauto'])\n",
    "    outwriter.writerow(['mean']+list(total_mean))\n",
    "    outwriter.writerow(['std']+list(total_std))\n",
    "    outwriter.writerow(['median']+list(total_median))\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "\n",
    "#r_norm = np.linalg.norm(rdata_statis,axis=0)\n",
    "r_max = np.max(rdata_statis,axis=0)\n",
    "r_min = np.min(rdata_statis, axis = 0)\n",
    "r_mean = np.mean(rdata_statis, axis = 0)\n",
    "rdata_statis_norm = np.zeros(np.shape(rdata_statis))\n",
    "for i in range(rdata_statis_norm.shape[1]):\n",
    "    rdata_statis_norm[:,i] = ( (rdata_statis[:,i]-r_mean[i]) )/(r_max[i]-r_min[i])\n",
    "\n",
    "\n",
    "#f_norm = np.linalg.norm(fdata_statis,axis=0)\n",
    "f_max = np.max(fdata_statis,axis=0)\n",
    "f_min = np.min(fdata_statis, axis = 0)\n",
    "f_mean = np.mean(fdata_statis, axis = 0)\n",
    "fdata_statis_norm = np.zeros(np.shape(fdata_statis))\n",
    "for i in range(fdata_statis_norm.shape[1]):\n",
    "    fdata_statis_norm[:,i] = ( (fdata_statis[:,i]-f_mean[i]) )/(f_max[i]-f_min[i])\n",
    "\n",
    "print(rdata_statis_norm.shape)\n",
    "print(fdata_statis_norm.shape)\n",
    "\n",
    "\n",
    "# mix\n",
    "\n",
    "dataplot = []\n",
    "for i in range(9):\n",
    "    dataplot.append(rdata_statis_norm[:,i])\n",
    "    dataplot.append(fdata_statis_norm[:,i])\n",
    "dataplot = np.transpose(np.array(dataplot))\n",
    "print(dataplot.shape)\n",
    "\n",
    "labels=[]\n",
    "for i in range(1,10):\n",
    "    labels.append(str(i)+\"\")\n",
    "    labels.append('*')\n",
    "\n",
    "fig,axes = plt.subplots()\n",
    "axes.boxplot(x=dataplot,sym='rd',patch_artist=True,labels=labels)\n",
    "axes.set_xlabel('Feature')\n",
    "plt.text(14,0.9,'Num : Real')\n",
    "plt.text(15,0.8,'* : Generate')\n",
    "plt.savefig(save + '/' + \"boxplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
