{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "training1 setup : \n",
    "upsample = True\n",
    "nearest\n",
    "not use post_filter\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 調整參數\n",
    "fs = 400000 #400k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cyclos data\n",
    "import csv\n",
    "import numpy as np\n",
    "real_datas = []\n",
    "with open('cyclo2_fresh.csv', newline='') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    for row in rows:\n",
    "        real_datas.append( list( map(float,row) ) )\n",
    "    csvfile.close()\n",
    "real_datas = np.array(real_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_num = 1\n",
    "print(np.max(real_datas),np.min(real_datas),np.mean(real_datas))\n",
    "print(np.max(real_datas/finetune_num),np.min(real_datas/finetune_num),np.mean(real_datas/finetune_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_datas = real_datas / finetune_num\n",
    "print(np.max(real_datas),np.min(real_datas),np.mean(real_datas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "print( stats.describe(real_datas[0]) )\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(np.arange(0,len(real_datas[1])/fs,1/fs),real_datas[10])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # datas = np.array(datas)\n",
    "# # print(datas[0].shape)\n",
    "# plt.figure(figsize=(20,10))\n",
    "# r_h = plt.hist(datas[3],1000)\n",
    "# plt.clf()\n",
    "# plt.plot(r_h[1][:-1],r_h[0]/1000,color=(1,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Ipynb_importer\n",
    "# %load train.py\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch import optim\n",
    "import json\n",
    "#from utils import save_samples\n",
    "import numpy as np\n",
    "import pprint\n",
    "import pickle\n",
    "import datetime\n",
    "from wavegan import *\n",
    "from utils import *\n",
    "from logger import *\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "# =============Logger===============\n",
    "LOGGER = logging.getLogger('wavegan')\n",
    "LOGGER.setLevel(logging.DEBUG)\n",
    "\n",
    "LOGGER.info('Initialized logger.')\n",
    "init_console_logger(LOGGER)\n",
    "\n",
    "# =============Parameters===============\n",
    "args = parse_arguments()\n",
    "epochs = args['num_epochs']\n",
    "batch_size = args['batch_size']\n",
    "latent_dim = args['latent_dim']\n",
    "ngpus = args['ngpus']\n",
    "model_size = args['model_size']\n",
    "model_dir = make_path(os.path.join(args['output_dir'],\n",
    "                                   datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")))\n",
    "args['model_dir'] = model_dir\n",
    "# save samples for every N epochs.\n",
    "epochs_per_sample = args['epochs_per_sample']\n",
    "# gradient penalty regularization factor.\n",
    "lmbda = args['lmbda']\n",
    "\n",
    "# Dir\n",
    "audio_dir = args['audio_dir']\n",
    "output_dir = args['output_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============Reload--Network===============\n",
    "\n",
    "netG = WaveGANGenerator(model_size=model_size, ngpus=ngpus, latent_dim=latent_dim, upsample=True)\n",
    "netD = WaveGANDiscriminator(model_size=model_size, ngpus=ngpus)\n",
    "\n",
    "if cuda:\n",
    "    netG = torch.nn.DataParallel(netG).cuda()\n",
    "    netD = torch.nn.DataParallel(netD).cuda()\n",
    "    \n",
    "#netD_path = os.path.join(output_dir, \"200discriminator.pkl\")\n",
    "#netG_path = os.path.join(output_dir, \"200generator.pkl\")\n",
    "\n",
    "#netG.load_state_dict(torch.load(netG_path), strict=False)\n",
    "#netD.load_state_dict(torch.load(netD_path), strict=False)\n",
    "\n",
    "\n",
    "\n",
    "# \"Two time-scale update rule\"(TTUR) to update netD 4x faster than netG.\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args['learning_rate'], betas=(args['beta1'], args['beta2']))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=args['learning_rate'], betas=(args['beta1'], args['beta2']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample noise used for generated output.\n",
    "sample_noise = torch.randn(args['sample_size'], latent_dim)\n",
    "if cuda:\n",
    "    sample_noise = sample_noise.cuda()\n",
    "sample_noise_Var = autograd.Variable(sample_noise, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save config.\n",
    "LOGGER.info('Saving configurations...')\n",
    "config_path = os.path.join(model_dir, 'config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(args, f)\n",
    "\n",
    "#---------------------------------------------修改-------------------------------------------#\n",
    "    \n",
    "# Loading original paper audio data\n",
    "# LOGGER.info('Loading original paper audio data...')\n",
    "# audio_paths = get_all_audio_filepaths(audio_dir) #list\n",
    "# train_data, valid_data, test_data, train_size = split_data(audio_paths, args['valid_ratio'],\n",
    "#                                                            args['test_ratio'], batch_size)\n",
    "\n",
    "LOGGER.info('Loading my datas ...')\n",
    "#datas = np.array(datas)\n",
    "train_data, valid_data, test_data, train_size = split_data1(real_datas, args['valid_ratio'],\n",
    "                                                           args['test_ratio'], batch_size)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "\n",
    "TOTAL_TRAIN_SAMPLES = train_size\n",
    "BATCH_NUM = TOTAL_TRAIN_SAMPLES // batch_size\n",
    "train_iter = iter(train_data)\n",
    "valid_iter = iter(valid_data)\n",
    "test_iter = iter(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============Train===============\n",
    "history = []\n",
    "D_costs_train = []\n",
    "D_wasses_train = []\n",
    "D_costs_valid = []\n",
    "D_wasses_valid = []\n",
    "G_costs = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "LOGGER.info('Starting training...EPOCHS={}, BATCH_SIZE={}, BATCH_NUM={}'.format(epochs, batch_size, BATCH_NUM))\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    LOGGER.info(\"{} Epoch: {}/{}\".format(time_since(start), epoch, epochs))\n",
    "\n",
    "    D_cost_train_epoch = []\n",
    "    D_wass_train_epoch = []\n",
    "    D_cost_valid_epoch = []\n",
    "    D_wass_valid_epoch = []\n",
    "    G_cost_epoch = []\n",
    "    for i in range(1, BATCH_NUM+1):\n",
    "        # Set Discriminator parameters to require gradients.\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = True\n",
    "  \n",
    "        one = torch.tensor(1, dtype=torch.float)\n",
    "        neg_one = one * -1\n",
    "        if cuda:\n",
    "            one = one.cuda()\n",
    "            neg_one = neg_one.cuda()\n",
    "        #############################\n",
    "        # (1) Train Discriminator\n",
    "        #############################\n",
    "        for iter_dis in range(5): #訓練五次 D 一次 G\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # Noise\n",
    "            noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
    "            if cuda:\n",
    "                noise = noise.cuda()\n",
    "            noise_Var = Variable(noise, requires_grad=False)\n",
    "            real_data_Var = numpy_to_var(next(train_iter)['X'], cuda) #*16384(np.array)\n",
    "\n",
    "\n",
    "            # a) compute loss contribution from real training data\n",
    "            D_real = netD(real_data_Var)\n",
    "            D_real = D_real.mean()  # avg loss (平均batch100的loss)\n",
    "            D_real.backward(neg_one)  # loss * -1\n",
    "\n",
    "            # b) compute loss contribution from generated data, then backprop.\n",
    "            fake = autograd.Variable(netG(noise_Var).data)\n",
    "            #print(np.shape(fake))\n",
    "            D_fake = netD(fake)\n",
    "            D_fake = D_fake.mean()\n",
    "            D_fake.backward(one)\n",
    "\n",
    "            # c) compute gradient penalty and backprop\n",
    "            \n",
    "            #################################################################################\n",
    "#             w_gra = D_real-D_fake\n",
    "#             gradient_penalty = calc_gradient_penalty_EEG( w_gra, netD, real_data_Var.data,\n",
    "#                                                      fake.data, batch_size, lmbda,\n",
    "#                                                      use_cuda=cuda)\n",
    "            #################################################################################\n",
    "            \n",
    "            gradient_penalty = calc_gradient_penalty(netD, real_data_Var.data,\n",
    "                                                     fake.data, batch_size, lmbda,\n",
    "                                                     use_cuda=cuda)\n",
    "            gradient_penalty.backward(one)\n",
    "\n",
    "            # Compute cost * Wassertein loss..\n",
    "            D_cost_train = D_fake - D_real + gradient_penalty\n",
    "            D_wass_train = D_real - D_fake \n",
    "\n",
    "            # Update gradient of discriminator.\n",
    "            optimizerD.step()\n",
    "\n",
    "            #############################\n",
    "            # (2) Compute Valid data\n",
    "            #############################\n",
    "            netD.zero_grad()\n",
    "\n",
    "            valid_data_Var = numpy_to_var(next(valid_iter)['X'], cuda)\n",
    "            D_real_valid = netD(valid_data_Var)\n",
    "            D_real_valid = D_real_valid.mean()  # avg loss\n",
    "\n",
    "            # b) compute loss contribution from generated data, then backprop.\n",
    "            fake_valid = netG(noise_Var)\n",
    "            D_fake_valid = netD(fake_valid)\n",
    "            D_fake_valid = D_fake_valid.mean()\n",
    "\n",
    "            # c) compute gradient penalty and backprop\n",
    "            gradient_penalty_valid = calc_gradient_penalty(netD, valid_data_Var.data,\n",
    "                                                           fake_valid.data, batch_size, lmbda,\n",
    "                                                           use_cuda=cuda)\n",
    "            # Compute metrics and record in batch history.\n",
    "            D_cost_valid = D_fake_valid - D_real_valid + gradient_penalty_valid\n",
    "            D_wass_valid = D_real_valid - D_fake_valid\n",
    "\n",
    "            if cuda:\n",
    "                D_cost_train = D_cost_train.cpu()\n",
    "                D_wass_train = D_wass_train.cpu()\n",
    "                D_cost_valid = D_cost_valid.cpu()\n",
    "                D_wass_valid = D_wass_valid.cpu()\n",
    "\n",
    "            # Record costs\n",
    "            D_cost_train_epoch.append(D_cost_train.data.numpy())\n",
    "            D_wass_train_epoch.append(D_wass_train.data.numpy())\n",
    "            D_cost_valid_epoch.append(D_cost_valid.data.numpy())\n",
    "            D_wass_valid_epoch.append(D_wass_valid.data.numpy())\n",
    "\n",
    "        #############################\n",
    "        # (3) Train Generator\n",
    "        #############################\n",
    "        # Prevent discriminator update.\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Reset generator gradients\n",
    "        netG.zero_grad()\n",
    "\n",
    "        # Noise\n",
    "        noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
    "        if cuda:\n",
    "            noise = noise.cuda()\n",
    "        noise_Var = Variable(noise, requires_grad=False) \n",
    "\n",
    "        fake = netG(noise_Var)\n",
    "        G = netD(fake)\n",
    "        G = G.mean()\n",
    "\n",
    "        # Update gradients.\n",
    "        G.backward(neg_one)\n",
    "        G_cost = -G\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Record costs\n",
    "        if cuda:\n",
    "            G_cost = G_cost.cpu()\n",
    "        G_cost_epoch.append(G_cost.data.numpy())\n",
    "\n",
    "        LOGGER.info(\"{} Epoch={} Batch: {}/{} D_c:{:.4f} | D_w:{:.4f} | G:{:.4f}\".format(time_since(start), epoch,\n",
    "                                                                                             i, BATCH_NUM,\n",
    "                                                                                             D_cost_train.data.numpy(),\n",
    "                                                                                             D_wass_train.data.numpy(),\n",
    "                                                                                             G_cost.data.numpy()))\n",
    "\n",
    "    # Save the average cost of batches in every epoch.\n",
    "    D_cost_train_epoch_avg = sum(D_cost_train_epoch) / float(len(D_cost_train_epoch))\n",
    "    D_wass_train_epoch_avg = sum(D_wass_train_epoch) / float(len(D_wass_train_epoch))\n",
    "    D_cost_valid_epoch_avg = sum(D_cost_valid_epoch) / float(len(D_cost_valid_epoch))\n",
    "    D_wass_valid_epoch_avg = sum(D_wass_valid_epoch) / float(len(D_wass_valid_epoch))\n",
    "    G_cost_epoch_avg = sum(G_cost_epoch) / float(len(G_cost_epoch))\n",
    "\n",
    "    D_costs_train.append(D_cost_train_epoch_avg)\n",
    "    D_wasses_train.append(D_wass_train_epoch_avg)\n",
    "    D_costs_valid.append(D_cost_valid_epoch_avg)\n",
    "    D_wasses_valid.append(D_wass_valid_epoch_avg)\n",
    "    G_costs.append(G_cost_epoch_avg)\n",
    "    \n",
    "\n",
    "    LOGGER.info(\"{} D_cost_train:{:.4f} | D_wass_train:{:.4f} | D_cost_valid:{:.4f} | D_wass_valid:{:.4f} | \"\n",
    "                \"G_cost:{:.4f}\".format(time_since(start),\n",
    "                                       D_cost_train_epoch_avg,\n",
    "                                       D_wass_train_epoch_avg,\n",
    "                                       D_cost_valid_epoch_avg,\n",
    "                                       D_wass_valid_epoch_avg,\n",
    "                                       G_cost_epoch_avg))\n",
    "\n",
    "    #Generate audio samples.\n",
    "#     if epoch % epochs_per_sample == 0:\n",
    "#         LOGGER.info(\"Generating samples...\")\n",
    "#         sample_out = netG(sample_noise_Var)\n",
    "#         if cuda:\n",
    "#             sample_out = sample_out.cpu()\n",
    "#         sample_out = sample_out.data.numpy()\n",
    "#         sample_out = np.squeeze(sample_out)*2.1\n",
    "#         save_and_plot(sample_out, epoch, output_dir)\n",
    "    if epoch % 20 == 0 :\n",
    "        # Save model\n",
    "        LOGGER.info(\"Saving models...\")\n",
    "        netD_path = os.path.join(output_dir, str(epoch)+\"discriminator.pkl\")\n",
    "        netG_path = os.path.join(output_dir, str(epoch)+\"generator.pkl\")\n",
    "        torch.save(netD.state_dict(), netD_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        torch.save(netG.state_dict(), netG_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "    # TODO\n",
    "    # Early stopping by Inception Score(IS)\n",
    "\n",
    "LOGGER.info('>>>>>>>Training finished !<<<<<<<')\n",
    "\n",
    "# Save model\n",
    "LOGGER.info(\"Saving models...\")\n",
    "netD_path = os.path.join(output_dir, \"discriminator.pkl\")\n",
    "netG_path = os.path.join(output_dir, \"generator.pkl\")\n",
    "torch.save(netD.state_dict(), netD_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "torch.save(netG.state_dict(), netG_path, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Plot loss curve.\n",
    "LOGGER.info(\"Saving loss curve...\")\n",
    "plot_loss(D_costs_train, D_wasses_train,\n",
    "          D_costs_valid, D_wasses_valid, G_costs, output_dir)\n",
    "\n",
    "LOGGER.info(\"All finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control latent vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_and_plot(sample_out, epoch, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control the noise to see whethere generate signal change\n",
    "noise = torch.Tensor(1, latent_dim).uniform_(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise[0][0] = 0.7998\n",
    "noise[0][1] = 0.7541\n",
    "noise[0][6] = -0.3339\n",
    "noise[0][10] = -0.4920\n",
    "noise[0][50] = 0.4578\n",
    "noise[0][99] = -0.5574\n",
    "print(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(-1,1,0.2)\n",
    "print(a.shape)\n",
    "b = np.arange(-100,100,20)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "for i in range(len(b)):\n",
    "    #noise[0][99] = b[i]\n",
    "    ##noise[0][1] = 0.7541\n",
    "    #print(noise)\n",
    "\n",
    "    if cuda:\n",
    "        noise = noise.cuda()\n",
    "    noise_Var = Variable(noise, requires_grad=False)\n",
    "    sample_out = netG(noise_Var)  \n",
    "    if cuda:\n",
    "        sample_out = sample_out.cpu()\n",
    "    sample_out = sample_out.data.numpy() \n",
    "    sample_out = np.squeeze(sample_out)\n",
    "\n",
    "#     print(sample_out.shape)\n",
    "    print(stats.describe( sample_out ))\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.plot(sample_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_out.shape)\n",
    "print(stats.describe( sample_out ))\n",
    "plt.plot(sample_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_out.shape)\n",
    "print(stats.describe( sample_out ))\n",
    "plt.plot(sample_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################validate######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成雜訊\n",
    "LOGGER.info(\"Generating samples...\")\n",
    "fake_datas = []\n",
    "for i in range(245):\n",
    "    noise = torch.Tensor(batch_size, latent_dim).uniform_(-1, 1)\n",
    "    if cuda:\n",
    "        noise = noise.cuda()\n",
    "    noise_Var = Variable(noise, requires_grad=False)\n",
    "    sample_out = netG(noise_Var)  \n",
    "    if cuda:\n",
    "        sample_out = sample_out.cpu()\n",
    "    sample_out = sample_out.data.numpy() \n",
    "    sample_out = np.squeeze(sample_out)\n",
    "    fake_datas.append(sample_out)\n",
    "fake_datas = np.reshape( np.array(fake_datas),(-1,16384) )\n",
    "fake_datas = np.array(fake_datas)\n",
    "print(fake_datas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "#real_datas = np.array(real_datas[:15000]*finetune_num)\n",
    "fake_datas = np.array(fake_datas[:15000]*finetune_num)\n",
    "#print(real_datas.shape)\n",
    "print(fake_datas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(real_datas),np.min(real_datas),np.mean(real_datas))\n",
    "print(np.max(real_datas/finetune_num),np.min(real_datas/finetune_num),np.mean(real_datas/finetune_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(fake_datas),np.min(fake_datas),np.mean(fake_datas))\n",
    "print(np.max(fake_datas/finetune_num),np.min(fake_datas/finetune_num),np.mean(fake_datas/finetune_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9161687524867435 -0.8858108176145915 1.9927732840374616e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new folder to store metrice\n",
    "\n",
    "save = 'metrice'\n",
    "if not os.path.isdir(save):\n",
    "    os.makedirs(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = save + \"/\" + \"w_nearest_ep200\" + \"(cyclo2_fresh)\" + \".csv\"\n",
    "with open(a,'w',newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    for i in range(len(fake_datas)):\n",
    "        csvWriter.writerow( fake_datas[i] ) \n",
    "        \n",
    "\n",
    "        \n",
    "# b = save + \"/\" + \"real\" + \"(cyclo2_fresh)\" + \".csv\"\n",
    "# with open(b,'w',newline='') as csvFile:\n",
    "#     csvWriter = csv.writer(csvFile)\n",
    "#     for i in x:\n",
    "#         csvWriter.writerow( real_datas[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paremeter\n",
    "fs = 400000 # 400k\n",
    "data_length = len(real_datas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw time series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#fs = 360000 # 360k\n",
    "t = data_length / fs # sampling freq * sampling time = sample num\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot( np.arange(0,t,1/fs), real_datas[500], label = 'Real',color = (1,0,0))\n",
    "plt.legend(loc = 'upper right',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Noise amplitude(v)\",fontsize=16)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot( np.arange(0,t,1/fs), fake_datas[2], label = 'Generate')\n",
    "plt.legend(loc = 'upper right',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Noise amplitude(v)\",fontsize=16)\n",
    "\n",
    "\n",
    "plt.savefig(save + '/' + \"time_series.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(1,15000, size=(10))\n",
    "tot = 1\n",
    "# draw random 10 time series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#fs = 360000 # 360k\n",
    "t = data_length / fs # sampling freq * sampling time = sample num\n",
    "\n",
    "plt.figure(figsize=(15,50))\n",
    "for i in x:\n",
    "    plt.subplot(20,1,tot)\n",
    "    plt.plot( np.arange(0,t,1/fs), real_datas[i], label = 'Real',color = (1,0,0))\n",
    "    plt.legend(loc = 'upper right',fontsize=16)\n",
    "    plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "    plt.ylabel(\"Noise amplitude(v)\",fontsize=10)\n",
    "    \n",
    "    tot += 1\n",
    "    \n",
    "    plt.subplot(20,1,tot)\n",
    "    plt.plot( np.arange(0,t,1/fs), fake_datas[i], label = 'Generate')\n",
    "    plt.legend(loc = 'upper right',fontsize=16)\n",
    "    plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "    plt.ylabel(\"Noise amplitude(v)\",fontsize=10)\n",
    "    \n",
    "    tot += 1\n",
    "    \n",
    "plt.savefig(save + '/' + \"time_series(10random).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw stft\n",
    "from scipy import signal\n",
    "# fs = 360000\n",
    "f1, t1, Zxx1 = signal.stft(real_datas[500], fs=fs, nperseg=257, noverlap=128)\n",
    "f2, t2, Zxx2 = signal.stft(fake_datas[1], fs=fs, nperseg=257, noverlap=128)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "cm=plt.cm.get_cmap('plasma')\n",
    "plt.figure(figsize=(20,10))\n",
    "#plt.title(\" X:Time[s], Y:Frequency[KHz]\")\n",
    "plt.subplot(2,1,1)\n",
    "plt.pcolormesh(t1, f1/1000, abs(Zxx1),cmap=cm)\n",
    "plt.colorbar()\n",
    "#plt.legend(loc = 'best',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Frequency(kHz)\",fontsize=16)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.pcolormesh(t2, f2/1000, abs(Zxx2),cmap=cm)\n",
    "plt.colorbar()\n",
    "#plt.legend(loc = 'best',fontsize=16)\n",
    "plt.xlabel(\"Time(s)\",fontsize=16)\n",
    "plt.ylabel(\"Frequency(kHz)\",fontsize=16)\n",
    "plt.savefig(save + '/' + \"stft.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency value\n",
    "print(np.where(Zxx1>0.20))\n",
    "print(f1[43]/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft,ifft\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "le = len(real_datas[2])\n",
    "xf = np.linspace(0,fs,data_length) # 頻率\n",
    "xf2 = xf[:(le//2)]\n",
    "r_yf2 = abs(fft(fake_datas[100]))[:(le//2)]\n",
    "plt.plot(xf2,r_yf2)\n",
    "print(r_yf2.shape)\n",
    "print(xf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average FFT\n",
    "\n",
    "from scipy.fftpack import fft,ifft\n",
    "r_average_fft = np.zeros(len(real_datas[0]))\n",
    "f_average_fft = np.zeros(len(fake_datas[0]))\n",
    "le = len(real_datas)\n",
    "for i in range(len(real_datas)):\n",
    "    r_average_fft += abs( fft(real_datas[i]) ) / le\n",
    "    f_average_fft += abs( fft(fake_datas[i]) ) / le\n",
    "    \n",
    "ln = len(real_datas[0])\n",
    "import matplotlib.pyplot as plt\n",
    "r_yf2 = r_average_fft[ : (ln//2) ]\n",
    "f_yf2 = f_average_fft[ : (ln//2) ]\n",
    "xf = np.linspace(0,fs,data_length) # 頻率\n",
    "xf2 = xf[:(ln//2)] / 1000\n",
    "\n",
    "#print( r_average_fft.shape)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(xf2,r_yf2,label = 'Real',color=(1,0,0))\n",
    "plt.plot(xf2,f_yf2,label = 'Generate',alpha=0.5)\n",
    "plt.legend(loc='best',fontsize=16)\n",
    "plt.title('FFT of Mixed wave',fontsize=20,color='#F08080')\n",
    "plt.xlabel(\"Frequency(kHz)\",fontsize=16)\n",
    "plt.ylabel(\"Average FFT\",fontsize=16)\n",
    "\n",
    "plt.savefig(save + '/' + \"fft.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( np.where(f_yf2>200) )\n",
    "print( xf[2707]/1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statisitc - easy version\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def feature_change(chandata):\n",
    "    total_data = []\n",
    "    print(len(chandata))\n",
    "    for i in range(len(chandata)):\n",
    "        print(i)\n",
    "        # statistic data\n",
    "        nobs,(damin,damax),damean,davar,daskew,dakurto = stats.describe( chandata[i] )\n",
    "        total_data.append( [damax, damean, davar**(1/2), daskew, dakurto] )\n",
    "    return np.array( total_data )\n",
    "\n",
    "rdata_statis = feature_change(real_datas)\n",
    "fdata_statis = feature_change(fake_datas)\n",
    "\n",
    "fdata_statis = fdata_statis[:15675]\n",
    "print(rdata_statis.shape)\n",
    "print(fdata_statis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real datas easy version\n",
    "\n",
    "total_mean = np.mean(rdata_statis, axis = 0)\n",
    "total_std = np.std(rdata_statis, axis = 0)\n",
    "total_median = np.median(rdata_statis, axis = 0)\n",
    "\n",
    "print(\"max[v], mean[mv], std[v], skewness, kurtosis\")\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "print(total_median)\n",
    "\n",
    "with open('real_statistic.csv','w',newline=\"\") as csvfile:\n",
    "    outwriter = csv.writer(csvfile)\n",
    "    outwriter.writerow(['feature']+['max[v]', 'mean[mv]', 'std[v]', 'skewness', 'kurtosis'])\n",
    "    outwriter.writerow(['mean']+list(total_mean))\n",
    "    outwriter.writerow(['std']+list(total_std))\n",
    "    outwriter.writerow(['median']+list(total_median))\n",
    "    csvfile.close()\n",
    "    \n",
    "\n",
    "#fake datas\n",
    "\n",
    "total_mean = np.mean(fdata_statis, axis = 0)\n",
    "total_std = np.std(fdata_statis, axis = 0)\n",
    "total_median = np.median(fdata_statis, axis = 0)\n",
    "\n",
    "print(\"max[v], mean[mv],std[v], skewness, kurtosis\")\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "print(total_median)\n",
    "    \n",
    "with open('fake_statistic.csv','w',newline=\"\") as csvfile:\n",
    "    outwriter = csv.writer(csvfile)\n",
    "    outwriter.writerow(['feature']+['max[v]', 'mean[mv]', 'std[v]', 'skewness', 'kurtosis'])\n",
    "    outwriter.writerow(['mean']+list(total_mean))\n",
    "    outwriter.writerow(['std']+list(total_std))\n",
    "    outwriter.writerow(['median']+list(total_median))\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     print(\"real\")\n",
    "#     nobs,(damin,damax),damean,davar,daskew,dakurto = stats.describe( real_datas[i] )\n",
    "#     print([damax, damean*1000, davar**(1/2), daskew, dakurto])\n",
    "#     print(\"fake\")\n",
    "#     nobs,(damin,damax),damean,davar,daskew,dakurto = stats.describe( fake_datas[i] )\n",
    "#     print([damax, damean*1000, davar**(1/2), daskew, dakurto])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xf2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statisitc\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def autocorr(x):\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    return result[result.size//2:]\n",
    "\n",
    "def feature_change(chandata):\n",
    "    total_data = []\n",
    "    print(len(chandata))\n",
    "    for i in range(len(chandata)):\n",
    "        print(i)\n",
    "        # statistic data\n",
    "        nobs,(damin,damax),damean,davar,daskew,dakurto = stats.describe( chandata[i] )\n",
    "        # energy\n",
    "        E_fft = np.sum(chandata[i] ** 2) / len(chandata[i])\n",
    "        # auto statistic data\n",
    "        nobs,(aumin,aumax),aumean,auvar,auskew,aukurto = stats.describe( autocorr(chandata[i]) ) \n",
    "        # peak > 0.05\n",
    "        peaks, _ = find_peaks(chandata[i], height=0.05)\n",
    "        \n",
    "        total_data.append( [damax, damean, E_fft, davar**(1/2), daskew, dakurto, int(len(peaks)), auskew, aukurto] )\n",
    "    return np.array( total_data )\n",
    "\n",
    "rdata_statis = feature_change(real_datas)\n",
    "fdata_statis = feature_change(fake_datas)\n",
    "\n",
    "print(rdata_statis.shape)\n",
    "print(fdata_statis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store random 100 datas to generate CPS and coherence\n",
    "\n",
    "a = save + \"/\" + \"w_nearest_ep_statistic\" + \"(cyclo2_fresh)\" + \".csv\"\n",
    "with open(a,'w',newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    for i in range(len(fdata_statis)):\n",
    "        csvWriter.writerow( fdata_statis[i] ) \n",
    "        \n",
    "\n",
    "        \n",
    "b = save + \"/\" + \"real_statistic\" + \"(cyclo2_fresh)\" + \".csv\"\n",
    "with open(b,'w',newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    for i in range(len(rdata_statis)):\n",
    "        csvWriter.writerow( rdata_statis[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real datas\n",
    "\n",
    "total_mean = np.mean(rdata_statis, axis = 0)\n",
    "total_std = np.std(rdata_statis, axis = 0)\n",
    "total_median = np.median(rdata_statis, axis = 0)\n",
    "\n",
    "print(\"max[v], mean[v], energy[J], std[v], skewness, kurtosis, peak>0.05, skewauto, kuroauto\")\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "print(total_median)\n",
    "\n",
    "with open(save + '/' + 'real_statistic.csv','w',newline=\"\") as csvfile:\n",
    "    outwriter = csv.writer(csvfile)\n",
    "    outwriter.writerow(['feature']+['max[v]', 'mean[v]', 'energy[J]', 'std[v]', 'skewness', 'kurtosis', 'peak>0.05', 'skewauto', 'kuroauto'])\n",
    "    outwriter.writerow(['mean']+list(total_mean))\n",
    "    outwriter.writerow(['std']+list(total_std))\n",
    "    outwriter.writerow(['median']+list(total_median))\n",
    "    csvfile.close()\n",
    "    \n",
    "\n",
    "#fake datas\n",
    "\n",
    "total_mean = np.mean(fdata_statis, axis = 0)\n",
    "total_std = np.std(fdata_statis, axis = 0)\n",
    "total_median = np.median(fdata_statis, axis = 0)\n",
    "\n",
    "print(\"max[v], mean[v], energy[J], std[v], skewness, kurtosis, peak>0.05, skewauto, kuroauto\")\n",
    "print(total_mean)\n",
    "print(total_std)\n",
    "print(total_median)\n",
    "    \n",
    "with open(save + '/' + 'fake_statistic.csv','w',newline=\"\") as csvfile:\n",
    "    outwriter = csv.writer(csvfile)\n",
    "    outwriter.writerow(['feature']+['max[v]', 'mean[v]', 'energy[J]', 'std[v]', 'skewness', 'kurtosis', 'peak>0.05', 'skewauto', 'kuroauto'])\n",
    "    outwriter.writerow(['mean']+list(total_mean))\n",
    "    outwriter.writerow(['std']+list(total_std))\n",
    "    outwriter.writerow(['median']+list(total_median))\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "\n",
    "#r_norm = np.linalg.norm(rdata_statis,axis=0)\n",
    "r_max = np.max(rdata_statis,axis=0)\n",
    "r_min = np.min(rdata_statis, axis = 0)\n",
    "r_mean = np.mean(rdata_statis, axis = 0)\n",
    "rdata_statis_norm = np.zeros(np.shape(rdata_statis))\n",
    "for i in range(rdata_statis_norm.shape[1]):\n",
    "    rdata_statis_norm[:,i] = ( (rdata_statis[:,i]-r_mean[i]) )/(r_max[i]-r_min[i])\n",
    "\n",
    "\n",
    "#f_norm = np.linalg.norm(fdata_statis,axis=0)\n",
    "f_max = np.max(fdata_statis,axis=0)\n",
    "f_min = np.min(fdata_statis, axis = 0)\n",
    "f_mean = np.mean(fdata_statis, axis = 0)\n",
    "fdata_statis_norm = np.zeros(np.shape(fdata_statis))\n",
    "for i in range(fdata_statis_norm.shape[1]):\n",
    "    fdata_statis_norm[:,i] = ( (fdata_statis[:,i]-f_mean[i]) )/(f_max[i]-f_min[i])\n",
    "\n",
    "print(rdata_statis_norm.shape)\n",
    "print(fdata_statis_norm.shape)\n",
    "\n",
    "\n",
    "# mix\n",
    "\n",
    "dataplot = []\n",
    "for i in range(9):\n",
    "    dataplot.append(rdata_statis_norm[:,i])\n",
    "    dataplot.append(fdata_statis_norm[:,i])\n",
    "dataplot = np.transpose(np.array(dataplot))\n",
    "print(dataplot.shape)\n",
    "\n",
    "labels=[]\n",
    "for i in range(1,10):\n",
    "    labels.append(str(i)+\"\")\n",
    "    labels.append('*')\n",
    "\n",
    "fig,axes = plt.subplots()\n",
    "axes.boxplot(x=dataplot,sym='rd',patch_artist=True,labels=labels)\n",
    "axes.set_xlabel('Feature')\n",
    "plt.text(14,0.7,'Num : Real')\n",
    "plt.text(15,0.6,'* : Generate')\n",
    "plt.savefig(save + '/' + \"boxplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "\n",
    "#r_norm = np.linalg.norm(rdata_statis,axis=0)\n",
    "r_max = np.max(rdata_statis,axis=0)\n",
    "r_min = np.min(rdata_statis, axis = 0)\n",
    "r_mean = np.mean(rdata_statis, axis = 0)\n",
    "rdata_statis_norm = np.zeros(np.shape(rdata_statis))\n",
    "for i in range(rdata_statis_norm.shape[1]):\n",
    "    maxx = abs(r_max[i]) if abs(r_max[i])>abs(r_min[i]) else abs(r_min[i])\n",
    "    rdata_statis_norm[:,i] = ( (rdata_statis[:,i] )/maxx )\n",
    "\n",
    "\n",
    "#f_norm = np.linalg.norm(fdata_statis,axis=0)\n",
    "f_max = np.max(fdata_statis,axis=0)\n",
    "f_min = np.min(fdata_statis, axis = 0)\n",
    "f_mean = np.mean(fdata_statis, axis = 0)\n",
    "fdata_statis_norm = np.zeros(np.shape(fdata_statis))\n",
    "for i in range(fdata_statis_norm.shape[1]):\n",
    "    maxx = abs(f_max[i]) if abs(f_max[i])>abs(f_min[i]) else abs(f_min[i])\n",
    "    fdata_statis_norm[:,i] = ( (fdata_statis[:,i] )/maxx )\n",
    "\n",
    "print(rdata_statis_norm.shape)\n",
    "print(fdata_statis_norm.shape)\n",
    "\n",
    "\n",
    "# mix\n",
    "\n",
    "dataplot = []\n",
    "for i in range(9):\n",
    "    dataplot.append(rdata_statis_norm[:,i])\n",
    "    dataplot.append(fdata_statis_norm[:,i])\n",
    "dataplot = np.transpose(np.array(dataplot))\n",
    "print(dataplot.shape)\n",
    "\n",
    "labels=[]\n",
    "for i in range(1,10):\n",
    "    labels.append(str(i)+\"\")\n",
    "    labels.append('*')\n",
    "\n",
    "fig,axes = plt.subplots()\n",
    "axes.boxplot(x=dataplot,sym='rd',patch_artist=True,labels=labels)\n",
    "axes.set_xlabel('Feature')\n",
    "plt.text(14,-0.8,'Num : Real')\n",
    "plt.text(15,-0.9,'* : Generate')\n",
    "plt.savefig(save + '/' + \"boxplot1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for i in range(1,10):\n",
    "    labels.append(str(i))\n",
    "    labels.append('')\n",
    "print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
